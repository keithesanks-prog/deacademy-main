<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS Kinesis - Understanding Real-Time Data Streaming</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #30cfd0 0%, #330867 100%);
            color: #333;
            line-height: 1.8;
            padding: 20px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
            overflow: hidden;
        }

        header {
            background: linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%);
            color: white;
            padding: 50px;
            text-align: center;
        }

        header h1 {
            font-size: 3em;
            margin-bottom: 10px;
        }

        header p {
            font-size: 1.3em;
            opacity: 0.95;
        }

        .content {
            padding: 50px;
        }

        h2 {
            color: #203a43;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #30cfd0;
            font-size: 2em;
        }

        h3 {
            color: #2c5364;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.5em;
        }

        h4 {
            color: #38758a;
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }

        .analogy-box {
            background: linear-gradient(135deg, #e0f7fa 0%, #b2ebf2 100%);
            border-left: 6px solid #00acc1;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }

        .concept-box {
            background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
            border-left: 6px solid #ff6f00;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }

        .example-box {
            background: #f8fafc;
            border-left: 4px solid #10b981;
            padding: 25px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .stream-diagram {
            background: #1e293b;
            color: #cbd5e1;
            padding: 30px;
            border-radius: 10px;
            margin: 25px 0;
            font-family: 'Courier New', monospace;
            text-align: center;
            line-height: 2;
        }

        .shard-box {
            background: #06b6d4;
            color: white;
            padding: 15px;
            border-radius: 10px;
            margin: 10px;
            display: inline-block;
            min-width: 200px;
        }

        .arrow {
            color: #fbbf24;
            font-size: 1.5em;
            margin: 5px 0;
        }

        code {
            background: #1e293b;
            color: #fbbf24;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
            border: 2px solid #475569;
            line-height: 1.6;
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        th {
            background: #203a43;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 15px;
            border-bottom: 1px solid #e2e8f0;
        }

        tr:hover {
            background: #f8fafc;
        }

        ul,
        ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 12px;
        }

        strong {
            color: #203a43;
        }

        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 25px 0;
        }

        .comparison-column {
            background: #f8fafc;
            padding: 20px;
            border-radius: 10px;
            border-top: 4px solid #30cfd0;
        }

        .emoji {
            font-size: 1.3em;
        }

        .warning-box {
            background: #fee2e2;
            border-left: 6px solid #dc2626;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .success-box {
            background: #d1fae5;
            border-left: 6px solid #10b981;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }

        .spec-box {
            background: linear-gradient(135deg, #e8eaf6 0%, #c5cae9 100%);
            border-left: 6px solid #3f51b5;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }
    </style>
</head>

<body>
    <div class="container">
        <header>
            <h1>üåä AWS Kinesis</h1>
            <p>Understanding Real-Time Data Streaming from First Principles</p>
        </header>

        <div class="content">
            <h2><span class="emoji">üéØ</span> The Core Problem</h2>

            <div class="analogy-box">
                <h3>The Highway Analogy</h3>
                <p>Imagine tracking traffic in a city:</p>
                <ul>
                    <li><strong>‚ùå Batch Processing (Database):</strong> Every hour, someone drives around the city
                        counting cars, writes it down, then you analyze it. By the time you get the data, traffic has
                        changed.</li>
                    <li><strong>‚úÖ Stream Processing (Kinesis):</strong> Sensors on every road continuously send data
                        as cars pass. You see traffic patterns in real-time and can react immediately (reroute traffic,
                        predict jams).</li>
                </ul>

                <p><strong>Kinesis is a firehose of continuous data flowing at high speed.</strong></p>
            </div>

            <div class="concept-box">
                <h3>When You Need Streaming vs. Batch</h3>
                <table>
                    <tr>
                        <th>Use Batch Processing (RDS, S3)</th>
                        <th>Use Stream Processing (Kinesis)</th>
                    </tr>
                    <tr>
                        <td>Daily sales reports</td>
                        <td>Fraud detection on credit card transactions</td>
                    </tr>
                    <tr>
                        <td>Monthly analytics aggregation</td>
                        <td>Live dashboard of website clicks</td>
                    </tr>
                    <tr>
                        <td>Historical data analysis</td>
                        <td>IoT sensor monitoring (temperature, pressure)</td>
                    </tr>
                    <tr>
                        <td>Overnight ETL jobs</td>
            </div>

            <h2><span class="emoji">üß±</span> Core Concepts</h2> & Definitions</h2>

            <p>Understanding these terms will help you master Kinesis concepts throughout this guide:</p>

            <div class="spec-box">
                <h3>Core Kinesis Terminology</h3>

                <table>
                    <tr>
                        <th style="width: 25%;">Term</th>
                        <th style="width: 75%;">Definition</th>
                    </tr>
                    <tr>
                        <td><strong>Stream</strong></td>
                        <td>An ordered sequence of data records. Think of it as a continuously flowing river of data
                            where records arrive in order and are stored for a configurable retention period (24 hours
                            to 365 days).<br>
                            <em>Example:</em> <code>website-clickstream</code> is a stream that collects all user
                            clicks.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Shard</strong></td>
                        <td>A unit of capacity within a stream. Each shard provides 1 MB/s write and 2 MB/s read
                            throughput. Streams can have multiple shards to scale horizontally.<br>
                            <em>Example:</em> A stream with 5 shards can handle 5 MB/s of writes.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Record</strong></td>
                        <td>A single data item in a stream. Each record consists of a partition key, data blob (up to 1
                            MB), and a sequence number assigned by Kinesis.<br>
                            <em>Example:</em> <code>{"user_id": "123", "action": "click", "page": "/home"}</code>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Partition Key</strong></td>
                        <td>A key that determines which shard a record goes to. Kinesis uses MD5 hash of the partition
                            key to route records. Records with the same partition key always go to the same shard
                            (ensuring order).<br>
                            <em>Example:</em> Using <code>user_id</code> as partition key ensures all events from one
                            user are processed in order.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Sequence Number</strong></td>
                        <td>A unique identifier assigned to each record by Kinesis when it's added to a shard. Sequence
                            numbers are ordered - you can use them to track record order within a shard.<br>
                            <em>Example:</em> <code>49653353468219810426419072694234682215307642087422296114</code>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Producer</strong></td>
                        <td>Any application or service that sends (puts) data records into a Kinesis stream using
                            <code>put_record</code> or <code>put_records</code> API.<br>
                            <em>Example:</em> A web server logging user clicks, an IoT device sending sensor data.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Consumer</strong></td>
                        <td>Any application or service that reads (gets) data records from a Kinesis stream. Multiple
                            consumers can read from the same stream independently.<br>
                            <em>Example:</em> A Lambda function processing clickstream events, an analytics application.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Retention Period</strong></td>
                        <td>How long data records remain in a stream before being automatically deleted. Default is 24
                            hours, maximum is 365 days. Data can be read multiple times during this period.<br>
                            <em>Example:</em> With 7-day retention, you can replay data from last week.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Shard Iterator</strong></td>
                        <td>A pointer to a specific position in a shard. Used by consumers to track their reading
                            progress. Types include <code>LATEST</code> (newest records), <code>TRIM_HORIZON</code>
                            (oldest available), and <code>AT_SEQUENCE_NUMBER</code> (specific position).<br>
                            <em>Example:</em> Start reading from the oldest record with <code>TRIM_HORIZON</code>.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Enhanced Fan-Out</strong></td>
                        <td>A feature that gives each consumer a dedicated 2 MB/s read throughput per shard (instead of
                            sharing). Useful when you have many consumers reading from the same stream.<br>
                            <em>Example:</em> 5 consumers with enhanced fan-out each get 2 MB/s on each shard.
                        </td>
                    </tr>
                </table>
            </div>

            <div class="spec-box">
                <h3>AWS Services & Integration Terms</h3>

                <table>
                    <tr>
                        <th style="width: 25%;">Term</th>
                        <th style="width: 75%;">Definition</th>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Streams</strong></td>
                        <td>The core Kinesis service for real-time data ingestion and custom processing. You control
                            shards, retention, and consumer logic.<br>
                            <em>Use when:</em> You need custom processing, multiple consumers, or replay capability.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Analytics</strong></td>
                        <td>A managed service that lets you write SQL queries against streaming data in real-time. No
                            need to write custom processing code.<br>
                            <em>Use when:</em> You want to aggregate, filter, or transform data using SQL.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Firehose</strong></td>
                        <td>A fully managed service that automatically delivers streaming data to destinations like S3,
                            Redshift, or OpenSearch. Zero code required - just configure the destination.<br>
                            <em>Use when:</em> You just want to store or index streaming data without custom processing.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Lambda Event Source Mapping</strong></td>
                        <td>A configuration that automatically invokes a Lambda function when records arrive in a
                            Kinesis stream. AWS handles polling the stream and batching records for you.<br>
                            <em>Example:</em> Lambda automatically processes every batch of 100 records from your
                            stream.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>KCL (Kinesis Client Library)</strong></td>
                        <td>A library that simplifies building consumer applications. Handles shard distribution,
                            checkpointing, and failover automatically when running multiple consumer instances.<br>
                            <em>Use when:</em> Building a distributed consumer application with multiple EC2 instances.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>KPL (Kinesis Producer Library)</strong></td>
                        <td>A high-performance library for producing data to Kinesis. Provides automatic batching, retry
                            logic, and monitoring.<br>
                            <em>Use when:</em> You need high-throughput production with automatic optimization.
                        </td>
                    </tr>
                </table>
            </div>

            <div class="spec-box">
                <h3>Performance & Scaling Terms</h3>

                <table>
                    <tr>
                        <th style="width: 25%;">Term</th>
                        <th style="width: 75%;">Definition</th>
                    </tr>
                    <tr>
                        <td><strong>Throughput</strong></td>
                        <td>The amount of data that can be written to or read from a stream per second. Measured in MB/s
                            or records/s.<br>
                            <em>Example:</em> A stream with 10 shards has 10 MB/s write throughput.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Hot Shard / Hot Partition</strong></td>
                        <td>A shard that receives significantly more traffic than others, often due to poor partition
                            key choice. This can cause throttling since one shard hits its 1 MB/s limit while others are
                            idle.<br>
                            <em>Example:</em> Using <code>country="USA"</code> as partition key when 90% of users are
                            from USA.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Throttling</strong></td>
                        <td>When you exceed a shard's capacity limits (1 MB/s or 1,000 records/s for writes), Kinesis
                            rejects requests with <code>ProvisionedThroughputExceededException</code>.<br>
                            <em>Solution:</em> Add more shards or use better partition keys to distribute load.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Shard Splitting</strong></td>
                        <td>Dividing one shard into two to increase capacity. The original shard is closed and two new
                            shards are created, doubling the throughput.<br>
                            <em>Example:</em> Split a hot shard handling 1 MB/s into two shards each handling 0.5 MB/s.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Shard Merging</strong></td>
                        <td>Combining two shards into one to reduce cost when you have excess capacity. The two original
                            shards are closed and one new shard is created.<br>
                            <em>Example:</em> Merge two idle shards to save $0.015/hour per shard.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>On-Demand Mode</strong></td>
                        <td>A capacity mode where Kinesis automatically manages shards for you based on throughput. You
                            pay per GB of data instead of per shard-hour.<br>
                            <em>Use when:</em> Traffic is unpredictable or you don't want to manage shards manually.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Provisioned Mode</strong></td>
                        <td>A capacity mode where you manually specify and manage the number of shards. You pay per
                            shard-hour regardless of usage.<br>
                            <em>Use when:</em> Traffic is predictable and you want cost optimization through manual
                            scaling.
                        </td>
                    </tr>
                </table>
                </table>
            </div>

            <div class="analogy-box">
                <h3>üõ£Ô∏è Visual Analogy: The Multi-Lane Highway</h3>
                <p><strong>Understand Sharding instantly:</strong></p>
                <ul>
                    <li><strong>Stream = The Highway:</strong> The whole system connecting City A (Producer) to City B
                        (Consumer).</li>
                    <li><strong>Shard = A Lane:</strong> A highway can have 1 lane or 100 lanes. More lanes = more cars
                        (throughput).
                        <ul>
                            <li>Each lane can handle <strong>1,000 cars/second</strong> (1 MB/s).</li>
                            <li>If you have 5,000 cars/second, you need <strong>5 Lanes (Shards)</strong>.</li>
                        </ul>
                    </li>
                    <li><strong>Partition Key = Lane Assignment Logic:</strong>
                        <ul>
                            <li>Think of it as "Trucks must use Lane 1", "Cars use Lane 2".</li>
                            <li><strong>Same Key = Same Shard:</strong> If you use `User_ID` as the key, "User A" is
                                <em>always</em> in Lane 1. This guarantees their actions (Login -> View -> Checkout)
                                stay in order!</li>
                        </ul>
                    </li>
                    <li><strong>Shard Splitting = Adding a Lane:</strong> Widening the road to reduce traffic.</li>
                    <li><strong>Shard Merging = Closing a Lane:</strong> narrowing the road to save maintenance costs
                        (money) when traffic is low.</li>
                </ul>
            </div>

            <div class="spec-box">
                <h3>Data Processing Terms</h3>

                <table>
                    <tr>
                        <th style="width: 25%;">Term</th>
                        <th style="width: 75%;">Definition</th>
                    </tr>
                    <tr>
                        <td><strong>Batch Processing</strong></td>
                        <td>Processing data in large chunks at scheduled intervals (e.g., hourly, daily). Data is
                            collected and processed all at once.<br>
                            <em>Example:</em> Running a nightly job to analyze yesterday's sales data.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Stream Processing</strong></td>
                        <td>Processing data continuously as it arrives in real-time. Each record is processed
                            immediately or in small micro-batches.<br>
                            <em>Example:</em> Detecting credit card fraud as transactions happen.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Windowing</strong></td>
                        <td>Grouping streaming data into time-based chunks for aggregation. Common types: Tumbling
                            (fixed, non-overlapping), Sliding (overlapping), Session (based on activity gaps).<br>
                            <em>Example:</em> Count clicks per 5-minute window using
                            <code>TUMBLE(rowtime, INTERVAL '5' MINUTE)</code>.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Checkpointing</strong></td>
                        <td>Saving the current position (sequence number) in a stream so consumers can resume from where
                            they left off after a crash or restart.<br>
                            <em>Example:</em> KCL automatically saves checkpoints to DynamoDB.
                        </td>
                    </tr>
                    <tr>
                        <td><strong>Fan-Out Pattern</strong></td>
                        <td>An architecture where one stream feeds multiple independent consumers, each processing data
                            for different purposes.<br>
                            <em>Example:</em> One clickstream ‚Üí fraud detection + analytics + archival (3 consumers).
                        </td>
                    </tr>
                    <tr>
                        <td><strong>ETL (Extract, Transform, Load)</strong></td>
                        <td>A data pipeline pattern: Extract data from sources, Transform it (clean, enrich, aggregate),
                            Load it to destinations.<br>
                            <em>Example:</em> Extract from Kinesis ‚Üí Transform with Lambda ‚Üí Load to Redshift.
                        </td>
                    </tr>
                </table>
            </div>

            <div class="success-box">
                <h3>üí° Quick Reference for Beginners</h3>
                <p><strong>Most Important Terms to Remember:</strong></p>
                <ul>
                    <li><strong>Stream:</strong> The data highway where records flow</li>
                    <li><strong>Shard:</strong> A lane on that highway (more shards = more capacity)</li>
                    <li><strong>Partition Key:</strong> What determines which lane (shard) a record uses</li>
                    <li><strong>Producer:</strong> Sends data TO the stream</li>
                    <li><strong>Consumer:</strong> Reads data FROM the stream</li>
                    <li><strong>Record:</strong> One piece of data (like one click event)</li>
                </ul>
                <p><strong>When confused about a term, refer back to this glossary!</strong></p>
            </div>

            <h2><span class="emoji">üß©</span> Core Concepts</h2>

            <div class="concept-box">
                <h3>1. Stream = Continuous Flow of Data</h3>
                <p>A Kinesis stream is like a river of data records flowing continuously.</p>
                <pre><code>Stream: "website-clickstream"
‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì (data flowing constantly)
Record 1: {"user": "alice", "page": "/home", "timestamp": "..."}
Record 2: {"user": "bob", "page": "/product/123", "timestamp": "..."}
Record 3: {"user": "alice", "page": "/cart", "timestamp": "..."}
...</code></pre>
            </div>

            <div class="concept-box">
                <h3>2. Shard = Parallel Processing Lane</h3>
                <p>A shard is a unit of capacity. Think of it as a lane on a highway.</p>

                <div class="stream-diagram">
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;">
                        <div class="shard-box">
                            Shard 1<br>
                            <small>1 MB/s write<br>2 MB/s read</small>
                        </div>
                        <div class="shard-box">
                            Shard 2<br>
                            <small>1 MB/s write<br>2 MB/s read</small>
                        </div>
                        <div class="shard-box">
                            Shard 3<br>
                            <small>1 MB/s write<br>2 MB/s read</small>
                        </div>
                    </div>
                    <p style="margin-top: 20px;">Total Capacity: 3 MB/s write, 6 MB/s read</p>
                </div>

                <p><strong>Key Insight:</strong> More shards = more throughput, but also more cost!</p>
            </div>

            <div class="concept-box">
                <h3>3. Partition Key = Traffic Router</h3>
                <p>Determines which shard a record goes to (like assigning cars to lanes).</p>
                <pre><code># Good partition key: Evenly distributes load
partition_key = user_id  # Each user goes to different shard

# Bad partition key: All data to one shard (hotspot!)
partition_key = "constant"  # Everything to same shard</code></pre>

                <div class="warning-box">
                    <h4>‚ö†Ô∏è Hot Shard Problem</h4>
                    <p><strong>What:</strong> One shard gets overwhelmed while others sit idle.</p>
                    <p><strong>Cause:</strong> Poor partition key choice (e.g., using <code>country</code> as key when
                        80% of users are from USA).</p>
                    <p><strong>Solution:</strong> Use high-cardinality keys like <code>user_id</code> or
                        <code>device_id</code>.
                    </p>
                </div>
            </div>

            <div class="concept-box">
                <h3>4. Producer = Data Sender</h3>
                <p>Any application that puts records into the stream.</p>
                <ul>
                    <li>Web server logging clicks</li>
                    <li>IoT devices sending sensor data</li>
                    <li>Mobile app tracking events</li>
                </ul>
            </div>

            <div class="concept-box">
                <h3>5. Consumer = Data Processor</h3>
                <p>Application that reads and processes records from the stream.</p>
                <ul>
                    <li>Lambda function analyzing events</li>
                    <li>Analytics service aggregating metrics</li>
                    <li>Fraud detection system flagging suspicious transactions</li>
                </ul>
            </div>

            <h2><span class="emoji">üîÑ</span> How AWS Kinesis Works?</h2>

            <p>Understanding the complete data lifecycle in Kinesis - from ingestion to consumption:</p>

            <div class="stream-diagram">
                <div style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 10px;">
                    <div style="background: #10b981; color: white; padding: 15px; border-radius: 5px;">
                        <strong>1. Data Ingestion</strong><br>
                        <small>Collect from sources</small>
                    </div>
                    <div style="background: #06b6d4; color: white; padding: 15px; border-radius: 5px;">
                        <strong>2. Sharding & Scaling</strong><br>
                        <small>Distribute to shards</small>
                    </div>
                    <div style="background: #8b5cf6; color: white; padding: 15px; border-radius: 5px;">
                        <strong>3. Processing & Buffering</strong><br>
                        <small>Prepare data</small>
                    </div>
                    <div style="background: #f59e0b; color: white; padding: 15px; border-radius: 5px;">
                        <strong>4. Data Access</strong><br>
                        <small>Consume & utilize</small>
                    </div>
                </div>
            </div>

            <h3>Step 1: Data Ingestion</h3>

            <div class="concept-box">
                <p><strong>What Happens:</strong> Amazon Kinesis receives data from multiple sources simultaneously.</p>

                <h4>Common Data Sources:</h4>
                <ul>
                    <li><strong>Applications:</strong> Web servers, mobile apps, backend services sending events</li>
                    <li><strong>IoT Sensors:</strong> Temperature, pressure, motion detectors streaming telemetry</li>
                    <li><strong>Log Producers:</strong> Application logs, server metrics, security logs</li>
                    <li><strong>Clickstream:</strong> User interactions, page views, button clicks</li>
                    <li><strong>Financial Data:</strong> Stock prices, transaction records, market data</li>
                </ul>

                <h4>Ingestion Methods:</h4>
                <pre><code># Method 1: Direct API (put_record)
kinesis.put_record(
    StreamName='sensor-data',
    Data=json.dumps({'temperature': 72.5}),
    PartitionKey='device-001'
)

# Method 2: Batch API (put_records) - More efficient
kinesis.put_records(
    StreamName='sensor-data',
    Records=[
        {'Data': json.dumps({'temp': 72.5}), 'PartitionKey': 'device-001'},
        {'Data': json.dumps({'temp': 73.1}), 'PartitionKey': 'device-002'},
        {'Data': json.dumps({'temp': 71.8}), 'PartitionKey': 'device-003'}
    ]
)

# Method 3: Kinesis Agent (for log files)
# Automatically tails log files and sends to stream

# Method 4: AWS SDK / Kinesis Producer Library (KPL)
# High-performance library with batching and retry logic</code></pre>

                <p><strong>Key Point:</strong> Producers don't need to know about shards - they just send data with a
                    partition key. Kinesis handles the routing.</p>
            </div>

            <h3>Step 2: Sharding and Scaling</h3>

            <div class="concept-box">
                <p><strong>What Happens:</strong> Incoming data is distributed across shards based on partition keys.
                    Kinesis automatically routes each record to the appropriate shard.</p>

                <h4>How Partition Key Determines Shard:</h4>
                <pre><code># Kinesis uses MD5 hash of partition key
partition_key = "user-12345"
hash_value = MD5(partition_key)
shard = hash_value % number_of_shards

# Example distribution:
# Partition Key: "user-001" ‚Üí Hash: 0x3A7F... ‚Üí Shard 0
# Partition Key: "user-002" ‚Üí Hash: 0x8B2C... ‚Üí Shard 1
# Partition Key: "user-003" ‚Üí Hash: 0x1D9E... ‚Üí Shard 2</code></pre>

                <div class="stream-diagram">
                    <div style="background: #10b981; color: white; padding: 10px; border-radius: 5px; margin: 10px;">
                        <strong>Incoming Data Stream</strong><br>
                        <small>1000 records/sec with different partition keys</small>
                    </div>
                    <div class="arrow">‚¨á (Hash partition key)</div>
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px;">
                        <div class="shard-box">
                            <strong>Shard 0</strong><br>
                            <small>~333 records/sec<br>user-001, user-004, user-007...</small>
                        </div>
                        <div class="shard-box">
                            <strong>Shard 1</strong><br>
                            <small>~333 records/sec<br>user-002, user-005, user-008...</small>
                        </div>
                        <div class="shard-box">
                            <strong>Shard 2</strong><br>
                            <small>~334 records/sec<br>user-003, user-006, user-009...</small>
                        </div>
                    </div>
                </div>

                <h4>Scaling Operations:</h4>
                <p><strong>Horizontal Scaling:</strong> Kinesis can scale shards up or down based on demand.</p>

                <ul>
                    <li><strong>Shard Splitting:</strong> Divide one shard into two (increase capacity)</li>
                    <li><strong>Shard Merging:</strong> Combine two shards into one (reduce cost)</li>
                    <li><strong>No Limit:</strong> You can have thousands of shards in a stream</li>
                </ul>

                <pre><code># Split a shard to double capacity
kinesis.split_shard(
    StreamName='sensor-data',
    ShardToSplit='shardId-000000000000',
    NewStartingHashKey='170141183460469231731687303715884105728'  # Midpoint
)

# Result: 
# Old: shardId-000000000000 (closed)
# New: shardId-000000000001 (first half)
#      shardId-000000000002 (second half)

# Merge two shards to reduce cost
kinesis.merge_shards(
    StreamName='sensor-data',
    ShardToMerge='shardId-000000000001',
    AdjacentShardToMerge='shardId-000000000002'
)

# Or use Auto Scaling with On-Demand mode
kinesis.update_stream_mode(
    StreamARN='arn:aws:kinesis:us-east-1:123456789012:stream/sensor-data',
    StreamModeDetails={'StreamMode': 'ON_DEMAND'}
)
# Kinesis now auto-scales based on throughput!</code></pre>

                <div class="warning-box">
                    <h4>‚ö†Ô∏è Shard Limit Awareness</h4>
                    <p><strong>Each shard can handle:</strong></p>
                    <ul>
                        <li>Write: 1 MB/s or 1,000 records/s (whichever comes first)</li>
                        <li>Read: 2 MB/s or 5 GetRecords API calls/s</li>
                    </ul>
                    <p><strong>Example:</strong> If you send 10,000 records/s, you need at least 10 shards (10,000 √∑
                        1,000).</p>
                </div>
            </div>

            <h3>Step 3: Processing and Buffering</h3>

            <div class="concept-box">
                <p><strong>What Happens:</strong> Data is stored in shards and prepared for consumption. Kinesis
                    maintains order within each shard and retains data for the configured retention period.</p>

                <h4>Data Retention and Ordering:</h4>
                <ul>
                    <li><strong>Retention:</strong> Data is kept for 24 hours (default) up to 365 days</li>
                    <li><strong>Ordering Guarantee:</strong> Records with the same partition key go to the same shard in
                        order</li>
                    <li><strong>Buffering:</strong> Data remains available for multiple consumers to read independently
                    </li>
                    <li><strong>Sequence Numbers:</strong> Each record gets a unique sequence number for tracking</li>
                </ul>

                <div class="stream-diagram">
                    <div style="text-align: left; padding: 20px;">
                        <strong>Shard 0 Buffer (ordered by time):</strong>
                        <div style="background: #334155; padding: 10px; margin: 10px 0; border-radius: 5px;">
                            <code style="color: #10b981;">Seq: 001</code> ‚îÇ {"user": "alice", "action": "login"} ‚îÇ
                            <small>T+0ms</small>
                        </div>
                        <div style="background: #334155; padding: 10px; margin: 10px 0; border-radius: 5px;">
                            <code style="color: #10b981;">Seq: 002</code> ‚îÇ {"user": "alice", "action": "view_page"} ‚îÇ
                            <small>T+100ms</small>
                        </div>
                        <div style="background: #334155; padding: 10px; margin: 10px 0; border-radius: 5px;">
                            <code style="color: #10b981;">Seq: 003</code> ‚îÇ {"user": "alice", "action": "add_to_cart"}
                            ‚îÇ <small>T+250ms</small>
                        </div>
                        <p style="margin-top: 10px;">
                            <small>‚Üë All records for user "alice" (same partition key) arrive in order</small>
                        </p>
                    </div>
                </div>

                <h4>Processing Options:</h4>
                <table>
                    <tr>
                        <th>Processing Method</th>
                        <th>Use Case</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td><strong>Raw Consumption</strong></td>
                        <td>Direct processing by custom app</td>
                        <td>Lambda reading records as-is</td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Analytics</strong></td>
                        <td>SQL queries on streaming data</td>
                        <td>Real-time aggregations, windowing</td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Firehose</strong></td>
                        <td>Automatic delivery to data lakes</td>
                        <td>Stream ‚Üí S3/Redshift/Elasticsearch</td>
                    </tr>
                    <tr>
                        <td><strong>KCL (Kinesis Client Library)</strong></td>
                        <td>Distributed processing with checkpointing</td>
                        <td>Multi-instance consumers with failover</td>
                    </tr>
                </table>

                <h4>Example: Buffering with Retention</h4>
                <pre><code># Set retention period
kinesis.increase_stream_retention_period(
    StreamName='sensor-data',
    RetentionPeriodHours=168  # 7 days
)

# Now consumers can read data from up to 7 days ago
# Useful for:
# - Replaying data after bug fixes
# - Late-joining consumers (new analytics service)
# - Disaster recovery scenarios</code></pre>
            </div>

            <h3>Step 4: Making the Data Accessible</h3>

            <div class="concept-box">
                <p><strong>What Happens:</strong> Processed data is made available to consumers through various
                    consumption patterns and delivery methods.</p>

                <h4>Consumption Patterns:</h4>

                <div class="comparison-grid">
                    <div class="comparison-column">
                        <h4>üìä Real-Time Analytics</h4>
                        <p><strong>Consumer:</strong> Lambda + CloudWatch</p>
                        <pre><code>Lambda reads stream
  ‚Üì
Processes records
  ‚Üì
Pushes metrics to CloudWatch
  ‚Üì
Dashboard updates in real-time</code></pre>
                    </div>

                    <div class="comparison-column">
                        <h4>üíæ Data Lake Storage</h4>
                        <p><strong>Consumer:</strong> Kinesis Firehose</p>
                        <pre><code>Firehose reads stream
  ‚Üì
Batches records
  ‚Üì
Delivers to S3 (Parquet)
  ‚Üì
Query with Athena</code></pre>
                    </div>

                    <div class="comparison-column">
                        <h4>üîî Alerting System</h4>
                        <p><strong>Consumer:</strong> Lambda + SNS</p>
                        <pre><code>Lambda reads stream
  ‚Üì
Detects anomalies
  ‚Üì
Publishes to SNS
  ‚Üì
Alerts sent to team</code></pre>
                    </div>
                </div>

                <h4>Access Methods:</h4>

                <p><strong>1. Direct Consumer (GetRecords API)</strong></p>
                <pre><code># Standard consumer - pulls data
shard_iterator = kinesis.get_shard_iterator(
    StreamName='sensor-data',
    ShardId='shardId-000000000000',
    ShardIteratorType='LATEST'
)['ShardIterator']

while True:
    response = kinesis.get_records(ShardIterator=shard_iterator)
    
    for record in response['Records']:
        process_record(record)
    
    shard_iterator = response['NextShardIterator']
    time.sleep(1)  # Poll interval</code></pre>

                <p><strong>2. Lambda Event Source Mapping (Recommended)</strong></p>
                <pre><code># AWS automatically polls Kinesis and invokes Lambda
# No polling code needed!

def lambda_handler(event, context):
    """Lambda receives batch of records"""
    for record in event['Records']:
        # Decode data
        data = json.loads(base64.b64decode(record['kinesis']['data']))
        
        # Process
        print(f"Received: {data}")
        
    return {'statusCode': 200}</code></pre>

                <p><strong>3. Kinesis Data Firehose (Automated Delivery)</strong></p>
                <pre><code># Create Firehose delivery stream
firehose.create_delivery_stream(
    DeliveryStreamName='sensor-to-s3',
    DeliveryStreamType='KinesisStreamAsSource',
    KinesisStreamSourceConfiguration={
        'KinesisStreamARN': 'arn:aws:kinesis:us-east-1:123456789012:stream/sensor-data',
        'RoleARN': 'arn:aws:iam::123456789012:role/firehose-role'
    },
    S3DestinationConfiguration={
        'BucketARN': 'arn:aws:s3:::my-data-lake',
        'Prefix': 'sensor-data/',
        'BufferingHints': {
            'SizeInMBs': 5,      # Batch size
            'IntervalInSeconds': 300  # Batch interval (5 min)
        }
    }
)
# Now data automatically flows: Kinesis ‚Üí Firehose ‚Üí S3!</code></pre>

                <p><strong>4. Kinesis Data Analytics (SQL on Streams)</strong></p>
                <pre><code>-- Real-time SQL query on streaming data
CREATE OR REPLACE STREAM "TEMP_STREAM" (
    device_id VARCHAR(50),
    temperature DOUBLE,
    event_time TIMESTAMP
);

CREATE OR REPLACE PUMP "STREAM_PUMP" AS 
  INSERT INTO "TEMP_STREAM"
  SELECT STREAM device_id, temperature, ROWTIME
  FROM "SOURCE_SQL_STREAM_001";

-- Aggregate: Average temperature per device over 5-minute windows
CREATE OR REPLACE STREAM "DESTINATION_SQL_STREAM" (
    device_id VARCHAR(50),
    avg_temperature DOUBLE,
    window_start TIMESTAMP,
    window_end TIMESTAMP
);

CREATE OR REPLACE PUMP "AGGREGATE_PUMP" AS
  INSERT INTO "DESTINATION_SQL_STREAM"
  SELECT STREAM 
    device_id,
    AVG(temperature) as avg_temperature,
    STEP(ROWTIME BY INTERVAL '5' MINUTE) as window_start,
    STEP(ROWTIME BY INTERVAL '5' MINUTE) + INTERVAL '5' MINUTE as window_end
  FROM "TEMP_STREAM"
  GROUP BY device_id, STEP(ROWTIME BY INTERVAL '5' MINUTE);</code></pre>

                <h4>Multiple Consumers Pattern:</h4>
                <div class="stream-diagram">
                    <div class="shard-box" style="width: 300px;">
                        <strong>Kinesis Stream</strong><br>
                        <small>sensor-data (3 shards)</small>
                    </div>
                    <div class="arrow">‚¨á ‚¨á ‚¨á ‚¨á</div>
                    <div style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 10px;">
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            <strong>Consumer 1</strong><br>
                            <small>Lambda: Real-time alerts</small>
                        </div>
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            <strong>Consumer 2</strong><br>
                            <small>Firehose: S3 archival</small>
                        </div>
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            <strong>Consumer 3</strong><br>
                            <small>Analytics: Dashboard</small>
                        </div>
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            <strong>Consumer 4</strong><br>
                            <small>ML Model: Training</small>
                        </div>
                    </div>
                    <p style="margin-top: 10px;"><small>‚Üë All consumers independently read the same data stream!</small>
                    </p>
                </div>

                <p><strong>Key Benefit:</strong> Same data powers multiple use cases simultaneously - real-time
                    monitoring, historical analysis, ML training, and archival.</p>
            </div>

            <h2><span class="emoji">üéÅ</span> Services Offered by AWS Kinesis</h2>

            <p>Kinesis is not just one service - it's a family of three integrated services that work together to
                provide
                a complete streaming data platform:</p>

            <h3>1. Kinesis Data Streams</h3>

            <div class="concept-box">
                <p><strong>Purpose:</strong> Provides a platform for real-time and continuous processing of data at
                    massive scale.</p>

                <h4>Key Features:</h4>
                <ul>
                    <li><strong>Real-Time Ingestion:</strong> Capture and store data streams in real-time</li>
                    <li><strong>Custom Processing:</strong> Build custom applications to process data exactly how you
                        need</li>
                    <li><strong>Durable Storage:</strong> Data retained for 24 hours to 365 days for replay</li>
                    <li><strong>Ordering Guarantee:</strong> Records with same partition key arrive in order</li>
                    <li><strong>Multiple Consumers:</strong> Same stream can feed multiple independent applications</li>
                </ul>

                <h4>When to Use:</h4>
                <table>
                    <tr>
                        <th>Use Case</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Custom real-time processing</td>
                        <td>Fraud detection with custom ML models</td>
                    </tr>
                    <tr>
                        <td>Multiple consumers needed</td>
                        <td>Feed same clickstream to analytics, ML, and archival</td>
                    </tr>
                    <tr>
                        <td>Need to replay data</td>
                        <td>Reprocess after bug fix or new algorithm</td>
                    </tr>
                    <tr>
                        <td>Strict ordering required</td>
                        <td>Process user actions in exact sequence</td>
                    </tr>
                </table>

                <h4>Code Example:</h4>
                <pre><code># Send data to Kinesis Data Streams
kinesis.put_record(
    StreamName='user-activity-stream',
    Data=json.dumps({
        'user_id': 'user-123',
        'action': 'purchase',
        'item_id': 'SKU-789',
        'timestamp': datetime.utcnow().isoformat()
    }),
    PartitionKey='user-123'
)

# Lambda automatically processes from stream
def lambda_handler(event, context):
    for record in event['Records']:
        data = json.loads(base64.b64decode(record['kinesis']['data']))
        # Custom processing logic here
        analyze_purchase(data)</code></pre>
            </div>

            <h3>2. Kinesis Data Analytics</h3>

            <div class="concept-box">
                <p><strong>Purpose:</strong> Allows you to analyze and process streams of data using standard SQL
                    queries in real-time - no need to learn a new language!</p>

                <h4>Key Features:</h4>
                <ul>
                    <li><strong>SQL Queries on Streams:</strong> Write familiar SQL instead of complex stream processing
                        code</li>
                    <li><strong>Real-Time Aggregations:</strong> Calculate averages, counts, sums as data flows</li>
                    <li><strong>Time Windows:</strong> Analyze data over sliding or tumbling time windows</li>
                    <li><strong>Anomaly Detection:</strong> Built-in ML algorithms for detecting outliers</li>
                    <li><strong>Managed Service:</strong> Fully managed - no servers to provision</li>
                </ul>

                <h4>When to Use:</h4>
                <table>
                    <tr>
                        <th>Use Case</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Real-time aggregations</td>
                        <td>Calculate average temperature per device every 5 minutes</td>
                    </tr>
                    <tr>
                        <td>Filtering streams</td>
                        <td>Only forward records where error_count > 10</td>
                    </tr>
                    <tr>
                        <td>Joining streams</td>
                        <td>Combine user clickstream with purchase events</td>
                    </tr>
                    <tr>
                        <td>Complex analytics without coding</td>
                        <td>Business analysts run SQL instead of writing Java/Python</td>
                    </tr>
                </table>

                <h4>SQL Example:</h4>
                <pre><code>-- Real-time aggregation: Count clicks per page every minute
CREATE OR REPLACE STREAM "CLICK_COUNTS" (
    page_url VARCHAR(200),
    click_count INTEGER,
    window_time TIMESTAMP
);

CREATE OR REPLACE PUMP "AGGREGATE_CLICKS" AS
  INSERT INTO "CLICK_COUNTS"
  SELECT STREAM 
    page_url,
    COUNT(*) as click_count,
    STEP(ROWTIME BY INTERVAL '1' MINUTE) as window_time
  FROM "SOURCE_SQL_STREAM"
  GROUP BY page_url, STEP(ROWTIME BY INTERVAL '1' MINUTE);

-- Filter high-error logs in real-time
CREATE OR REPLACE STREAM "HIGH_ERROR_ALERTS" (
    server VARCHAR(50),
    error_count INTEGER
);

CREATE OR REPLACE PUMP "ERROR_FILTER" AS
  INSERT INTO "HIGH_ERROR_ALERTS"
  SELECT STREAM server, COUNT(*) as error_count
  FROM "LOG_STREAM"
  WHERE log_level = 'ERROR'
  GROUP BY server, STEP(ROWTIME BY INTERVAL '5' MINUTE)
  HAVING COUNT(*) > 100;</code></pre>
            </div>

            <h3>3. Kinesis Data Firehose</h3>

            <div class="concept-box">
                <p><strong>Purpose:</strong> Reliably capture, transform, and load streaming data into AWS data stores
                    and analytics services - the easiest way to get data into S3, Redshift, or Elasticsearch.</p>

                <h4>Key Features:</h4>
                <ul>
                    <li><strong>Zero Code:</strong> No need to write consumers - just configure destinations</li>
                    <li><strong>Automatic Batching:</strong> Buffers and batches data for efficient delivery</li>
                    <li><strong>Data Transformation:</strong> Lambda functions can transform data before delivery</li>
                    <li><strong>Format Conversion:</strong> Convert JSON to Parquet/ORC for efficient querying</li>
                    <li><strong>Fully Managed:</strong> Auto-scales, no servers to manage</li>
                </ul>

                <h4>Supported Destinations:</h4>
                <table>
                    <tr>
                        <th>Destination</th>
                        <th>Use Case</th>
                        <th>Format Options</th>
                    </tr>
                    <tr>
                        <td><strong>Amazon S3</strong></td>
                        <td>Data lake, archival, long-term storage</td>
                        <td>JSON, Parquet, ORC</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon Redshift</strong></td>
                        <td>Data warehousing, SQL analytics</td>
                        <td>CSV, JSON (via S3 copy)</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon OpenSearch</strong></td>
                        <td>Log analytics, full-text search</td>
                        <td>JSON</td>
                    </tr>
                    <tr>
                        <td><strong>HTTP Endpoints</strong></td>
                        <td>Third-party services (Datadog, Splunk)</td>
                        <td>JSON</td>
                    </tr>
                </table>

                <h4>When to Use:</h4>
                <ul>
                    <li><strong>Zero Custom Code:</strong> Just want to dump stream into S3/Redshift/OpenSearch</li>
                    <li><strong>Format Conversion:</strong> Need JSON ‚Üí Parquet for efficient Athena queries</li>
                    <li><strong>Simple Transformations:</strong> Basic data enrichment or filtering with Lambda</li>
                    <li><strong>Automatic Archival:</strong> Store clickstream/logs with no effort</li>
                </ul>

                <h4>Code Example:</h4>
                <pre><code># Create Firehose delivery stream to S3
firehose.create_delivery_stream(
    DeliveryStreamName='clickstream-to-s3',
    S3DestinationConfiguration={
        'RoleARN': 'arn:aws:iam::123456789012:role/firehose-role',
        'BucketARN': 'arn:aws:s3:::my-data-lake',
        'Prefix': 'clickstream/year=!{timestamp:yyyy}/month=!{timestamp:MM}/',
        'BufferingHints': {
            'SizeInMBs': 128,          # Batch when 128 MB accumulated
            'IntervalInSeconds': 300    # Or every 5 minutes
        },
        'CompressionFormat': 'GZIP',    # Compress for cost savings
        'DataFormatConversionConfiguration': {
            'Enabled': True,
            'OutputFormatConfiguration': {
                'Serializer': {
                    'ParquetSerDe': {}  # Convert JSON ‚Üí Parquet
                }
            }
        }
    }
)

# Send data to Firehose (same API as Kinesis Streams)
firehose.put_record(
    DeliveryStreamName='clickstream-to-s3',
    Record={
        'Data': json.dumps({
            'user_id': 'user-123',
            'page': '/product/456',
            'timestamp': datetime.utcnow().isoformat()
        })
    }
)
# Data automatically lands in S3 as Parquet!</code></pre>
            </div>

            <h2><span class="emoji">üèóÔ∏è</span> Kinesis Architecture: Complete Example</h2>

            <p><strong>Scenario:</strong> Real-time content personalization system for a news website</p>

            <div class="stream-diagram">
                <div style="text-align: left; padding: 20px;">
                    <!-- Step 1 -->
                    <div style="background: #10b981; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>Step 1: Website Sends Clickstream Data</strong><br>
                        <small>Users browsing website ‚Üí Click events sent to Kinesis Data Streams</small>
                    </div>
                    <div class="arrow" style="text-align: center;">‚¨á</div>

                    <!-- Kinesis Data Streams -->
                    <div style="background: #06b6d4; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>üìä Kinesis Data Streams</strong><br>
                        <small>Stream: "website-clickstream" (5 shards)</small><br>
                        <small>Collects: user_id, article_id, timestamp, action (view, like, share)</small>
                    </div>
                    <div class="arrow" style="text-align: center;">‚¨á</div>

                    <!-- Step 2 -->
                    <div style="background: #8b5cf6; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>Step 2: Kinesis Data Analytics Processes in Real-Time</strong><br>
                        <small>SQL Query: Calculate trending articles (most views in last 10 minutes)</small>
                    </div>

                    <div
                        style="background: #334155; color: #e2e8f0; padding: 15px; border-radius: 10px; margin: 10px 0; font-family: monospace; font-size: 0.9em;">
                        -- Real-time trending calculation
                        SELECT article_id,
                        COUNT(*) as view_count,
                        AVG(engagement_score) as avg_engagement
                        FROM clickstream_input
                        WHERE action = 'view'
                        GROUP BY article_id,
                        STEP(ROWTIME BY INTERVAL '10' MINUTE)
                        HAVING COUNT(*) > 50
                        ORDER BY view_count DESC
                        LIMIT 10;
                    </div>
                    <div class="arrow" style="text-align: center;">‚¨á</div>

                    <!-- Step 3 -->
                    <div style="background: #f59e0b; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>Step 3: Kinesis Data Firehose Loads to Redshift</strong><br>
                        <small>Batches processed analytics results ‚Üí Delivers to Amazon Redshift every 5 minutes</small>
                    </div>
                    <div class="arrow" style="text-align: center;">‚¨á</div>

                    <!-- Step 4 -->
                    <div style="background: #dc2626; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>Step 4: Amazon Redshift Analytics</strong><br>
                        <small>Run complex analytics models on historical + real-time data</small><br>
                        <small>Generate personalized content recommendations per user</small>
                    </div>
                    <div class="arrow" style="text-align: center;">‚¨á</div>

                    <!-- Step 5 -->
                    <div style="background: #10b981; color: white; padding: 15px; border-radius: 10px; margin: 10px 0;">
                        <strong>Step 5: Personalized Content Served</strong><br>
                        <small>‚úÖ Readers see personalized article suggestions</small><br>
                        <small>‚úÖ Higher engagement (more clicks, longer sessions)</small><br>
                        <small>‚úÖ Increased ad revenue from better targeting</small>
                    </div>
                </div>
            </div>

            <h3>Comparison: When to Use Each Service</h3>

            <table>
                <tr>
                    <th>If You Need...</th>
                    <th>Use This</th>
                    <th>Why</th>
                </tr>
                <tr>
                    <td>Custom processing logic</td>
                    <td><strong>Kinesis Data Streams</strong> + Lambda/EC2</td>
                    <td>Full control over processing</td>
                </tr>
                <tr>
                    <td>SQL queries on streaming data</td>
                    <td><strong>Kinesis Data Analytics</strong></td>
                    <td>No code, just SQL</td>
                </tr>
                <tr>
                    <td>Store stream in S3/Redshift</td>
                    <td><strong>Kinesis Data Firehose</strong></td>
                    <td>Zero code, automatic delivery</td>
                </tr>
                <tr>
                    <td>Multiple consumers + complex processing</td>
                    <td><strong>Kinesis Data Streams</strong></td>
                    <td>Flexibility for diverse use cases</td>
                </tr>
                <tr>
                    <td>Real-time aggregations without coding</td>
                    <td><strong>Kinesis Data Analytics</strong></td>
                    <td>Business analysts can write SQL</td>
                </tr>
                <tr>
                    <td>Simple archival pipeline</td>
                    <td><strong>Kinesis Data Firehose</strong></td>
                    <td>Easiest, fully managed</td>
                </tr>
            </table>

            <div class="success-box">
                <h4>üí° Common Pattern: Use All Three Together!</h4>
                <p><strong>Flow:</strong> Data Streams (collect) ‚Üí Data Analytics (filter/aggregate) ‚Üí Data Firehose
                    (store results)</p>
                <ul>
                    <li><strong>Example 1:</strong> Collect IoT sensor data (Streams) ‚Üí Calculate anomalies with SQL
                        (Analytics) ‚Üí Store flagged devices (Firehose ‚Üí S3)</li>
                    <li><strong>Example 2:</strong> Clickstream (Streams) ‚Üí Real-time trending (Analytics) ‚Üí
                        Recommendations to Redshift (Firehose)</li>
                    <li><strong>Example 3:</strong> Application logs (Streams) ‚Üí Filter errors with SQL (Analytics) ‚Üí
                        Send to OpenSearch (Firehose)</li>
                </ul>
            </div>

            <h2><span class="emoji">üåê</span> Complete AWS Kinesis Ecosystem - How Everything Connects</h2>

            <p>Here's how all the Kinesis components, AWS services, and your applications work together to form powerful
                real-time data pipelines:</p>

            <h3>The Big Picture: Complete Data Flow</h3>

            <div class="stream-diagram">
                <div style="text-align: left; padding: 30px; font-size: 0.95em;">
                    <!-- LAYER 1: DATA SOURCES -->
                    <div style="background: #10b981; color: white; padding: 20px; border-radius: 10px; margin: 15px 0;">
                        <strong style="font-size: 1.2em;">üì• LAYER 1: DATA SOURCES (Producers)</strong>
                        <div
                            style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 10px; margin-top: 15px;">
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                <strong>Web/Mobile Apps</strong><br>
                                <small>User clicks, events</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                <strong>IoT Devices</strong><br>
                                <small>Sensor data</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                <strong>Application Logs</strong><br>
                                <small>Server metrics</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                <strong>DynamoDB Streams</strong><br>
                                <small>DB changes</small>
                            </div>
                        </div>
                    </div>

                    <div class="arrow" style="text-align: center; font-size: 2em;">‚¨á put_record / put_records</div>

                    <!-- LAYER 2: KINESIS DATA STREAMS -->
                    <div style="background: #06b6d4; color: white; padding: 20px; border-radius: 10px; margin: 15px 0;">
                        <strong style="font-size: 1.2em;">üåä LAYER 2: KINESIS DATA STREAMS (Ingestion Hub)</strong>
                        <div
                            style="margin-top: 15px; background: rgba(255,255,255,0.1); padding: 15px; border-radius: 5px;">
                            <strong>Stream: "my-data-stream"</strong>
                            <div
                                style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin-top: 10px;">
                                <div
                                    style="background: rgba(255,255,255,0.2); padding: 8px; border-radius: 5px; text-align: center;">
                                    <strong>Shard 1</strong><br>
                                    <small>1 MB/s write<br>2 MB/s read</small>
                                </div>
                                <div
                                    style="background: rgba(255,255,255,0.2); padding: 8px; border-radius: 5px; text-align: center;">
                                    <strong>Shard 2</strong><br>
                                    <small>1 MB/s write<br>2 MB/s read</small>
                                </div>
                                <div
                                    style="background: rgba(255,255,255,0.2); padding: 8px; border-radius: 5px; text-align: center;">
                                    <strong>Shard 3</strong><br>
                                    <small>1 MB/s write<br>2 MB/s read</small>
                                </div>
                            </div>
                            <div style="margin-top: 10px; font-size: 0.9em;">
                                ‚úì Retention: 24h - 365 days<br>
                                ‚úì Ordering per partition key<br>
                                ‚úì Multiple consumers can read same data
                            </div>
                        </div>
                    </div>

                    <div class="arrow" style="text-align: center; font-size: 1.5em;">‚¨á ‚¨á ‚¨á (3 parallel paths)</div>

                    <!-- LAYER 3: PROCESSING OPTIONS -->
                    <div style="background: #8b5cf6; color: white; padding: 20px; border-radius: 10px; margin: 15px 0;">
                        <strong style="font-size: 1.2em;">‚öôÔ∏è LAYER 3: PROCESSING & ROUTING</strong>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin-top: 15px;">
                            <!-- Path A: Analytics -->
                            <div style="background: rgba(255,255,255,0.15); padding: 15px; border-radius: 8px;">
                                <strong>üîç Path A: Real-Time Analytics</strong>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 10px; margin-top: 10px; border-radius: 5px; font-size: 0.85em;">
                                    <strong>Kinesis Data Analytics</strong><br>
                                    <small>SQL queries on stream</small>
                                    <pre
                                        style="background: rgba(0,0,0,0.3); padding: 5px; margin-top: 5px; font-size: 0.8em;">SELECT user_id,
  COUNT(*) as clicks
FROM stream
GROUP BY user_id,
TUMBLE(rowtime,
  INTERVAL '5' MINUTE)</pre>
                                </div>
                            </div>

                            <!-- Path B: Lambda Processing -->
                            <div style="background: rgba(255,255,255,0.15); padding: 15px; border-radius: 8px;">
                                <strong>‚ö° Path B: Custom Processing</strong>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 10px; margin-top: 10px; border-radius: 5px; font-size: 0.85em;">
                                    <strong>AWS Lambda</strong><br>
                                    <small>Event source mapping</small>
                                    <ul style="margin: 5px 0; padding-left: 20px; font-size: 0.9em;">
                                        <li>Fraud detection</li>
                                        <li>Data enrichment</li>
                                        <li>Alerting</li>
                                    </ul>
                                </div>
                            </div>

                            <!-- Path C: Direct Archival -->
                            <div style="background: rgba(255,255,255,0.15); padding: 15px; border-radius: 8px;">
                                <strong>üíæ Path C: Archival</strong>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 10px; margin-top: 10px; border-radius: 5px; font-size: 0.85em;">
                                    <strong>Kinesis Data Firehose</strong><br>
                                    <small>Zero-code delivery</small>
                                    <ul style="margin: 5px 0; padding-left: 20px; font-size: 0.9em;">
                                        <li>Auto batching</li>
                                        <li>Compression</li>
                                        <li>Format conversion</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="arrow" style="text-align: center; font-size: 1.5em;">‚¨á ‚¨á ‚¨á</div>

                    <!-- LAYER 4: DESTINATIONS -->
                    <div style="background: #f59e0b; color: white; padding: 20px; border-radius: 10px; margin: 15px 0;">
                        <strong style="font-size: 1.2em;">üéØ LAYER 4: DESTINATIONS & ACTIONS</strong>
                        <div
                            style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 10px; margin-top: 15px;">
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>Amazon S3</strong><br>
                                <small>Data lake<br>Long-term storage</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>Amazon Redshift</strong><br>
                                <small>Data warehouse<br>SQL analytics</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>OpenSearch</strong><br>
                                <small>Log analytics<br>Dashboards</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>Amazon SNS</strong><br>
                                <small>Alerts<br>Notifications</small>
                            </div>
                        </div>
                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 10px; margin-top: 10px;">
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>CloudWatch</strong><br>
                                <small>Metrics & Monitoring</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>SageMaker</strong><br>
                                <small>ML predictions</small>
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 12px; border-radius: 5px;">
                                <strong>Custom Apps</strong><br>
                                <small>API endpoints</small>
                            </div>
                        </div>
                    </div>

                    <div class="arrow" style="text-align: center; font-size: 1.5em;">‚¨á</div>

                    <!-- LAYER 5: OUTCOMES -->
                    <div style="background: #10b981; color: white; padding: 20px; border-radius: 10px; margin: 15px 0;">
                        <strong style="font-size: 1.2em;">‚ú® LAYER 5: BUSINESS OUTCOMES</strong>
                        <div
                            style="display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 10px; margin-top: 15px;">
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                ‚úÖ Real-time insights
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                ‚úÖ Fraud prevention
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                ‚úÖ Personalization
                            </div>
                            <div style="background: rgba(255,255,255,0.2); padding: 10px; border-radius: 5px;">
                                ‚úÖ Predictive maintenance
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <h3>Real-World Integration Patterns</h3>

            <div class="comparison-grid">
                <div class="comparison-column">
                    <h4>Pattern 1: Analytics Pipeline</h4>
                    <pre style="font-size: 0.85em;"><code>Web App
  ‚Üì (clicks)
Kinesis Stream
  ‚Üì
Data Analytics (SQL)
  ‚Üì (aggregates)
Firehose
  ‚Üì
Redshift
  ‚Üì
QuickSight Dashboard</code></pre>
                    <p><strong>Use:</strong> Real-time business intelligence</p>
                </div>

                <div class="comparison-column">
                    <h4>Pattern 2: Event Processing</h4>
                    <pre style="font-size: 0.85em;"><code>IoT Sensors
  ‚Üì (telemetry)
Kinesis Stream
  ‚Üì
Lambda (filter)
  ‚Üì (anomalies only)
SNS
  ‚Üì
Email/SMS Alerts
  +
DynamoDB (history)</code></pre>
                    <p><strong>Use:</strong> Anomaly detection & alerting</p>
                </div>

                <div class="comparison-column">
                    <h4>Pattern 3: Data Lake</h4>
                    <pre style="font-size: 0.85em;"><code>Multiple Sources
  ‚Üì
Kinesis Stream
  ‚Üì
Firehose (transform)
  ‚Üì (Parquet)
S3 Data Lake
  ‚Üì
Athena Queries
  +
EMR Processing</code></pre>
                    <p><strong>Use:</strong> Historical analysis</p>
                </div>
            </div>

            <h3>Example: E-Commerce Complete Pipeline</h3>

            <div class="example-box">
                <h4>Scenario: Real-time fraud detection + analytics + archival</h4>

                <div class="stream-diagram">
                    <div style="text-align: left; padding: 20px;">
                        <!-- Source -->
                        <div
                            style="background: #10b981; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;">
                            <strong>üõí E-Commerce Website</strong><br>
                            <small>Events: purchases, cart adds, page views<br>
                                100 events/second</small>
                        </div>
                        <div class="arrow" style="text-align: center;">‚¨á put_records (batch)</div>

                        <!-- Kinesis Stream -->
                        <div
                            style="background: #06b6d4; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;">
                            <strong>üìä Kinesis Data Stream: "ecommerce-events"</strong><br>
                            <small>5 shards, 24-hour retention<br>
                                Partition key: user_id (ensures order per user)</small>
                        </div>

                        <!-- 3 Parallel Consumers -->
                        <div class="arrow" style="text-align: center;">‚¨á ‚¨á ‚¨á (3 consumers reading same stream)</div>

                        <div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 10px 0;">
                            <!-- Consumer 1 -->
                            <div style="background: #8b5cf6; color: white; padding: 12px; border-radius: 8px;">
                                <strong>Consumer 1: Fraud Detection</strong><br>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 8px; margin-top: 8px; border-radius: 5px; font-size: 0.85em;">
                                    Lambda Function<br>
                                    ‚Üì<br>
                                    Check: velocity, location, amount<br>
                                    ‚Üì<br>
                                    If suspicious ‚Üí SNS alert<br>
                                    ‚Üì<br>
                                    Block transaction
                                </div>
                            </div>

                            <!-- Consumer 2 -->
                            <div style="background: #8b5cf6; color: white; padding: 12px; border-radius: 8px;">
                                <strong>Consumer 2: Real-Time Analytics</strong><br>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 8px; margin-top: 8px; border-radius: 5px; font-size: 0.85em;">
                                    Kinesis Data Analytics<br>
                                    ‚Üì<br>
                                    SQL: trending products<br>
                                    ‚Üì<br>
                                    Firehose ‚Üí Redshift<br>
                                    ‚Üì<br>
                                    Live dashboard
                                </div>
                            </div>

                            <!-- Consumer 3 -->
                            <div style="background: #8b5cf6; color: white; padding: 12px; border-radius: 8px;">
                                <strong>Consumer 3: Data Lake</strong><br>
                                <div
                                    style="background: rgba(0,0,0,0.2); padding: 8px; margin-top: 8px; border-radius: 5px; font-size: 0.85em;">
                                    Kinesis Firehose<br>
                                    ‚Üì<br>
                                    Convert JSON ‚Üí Parquet<br>
                                    ‚Üì<br>
                                    Amazon S3<br>
                                    ‚Üì<br>
                                    Athena queries
                                </div>
                            </div>
                        </div>

                        <!-- Results -->
                        <div class="arrow" style="text-align: center;">‚¨á</div>
                        <div
                            style="background: #10b981; color: white; padding: 15px; border-radius: 8px; margin: 10px 0;">
                            <strong>‚ú® Business Impact:</strong><br>
                            ‚Ä¢ Fraud blocked in < 100ms (before payment processes)<br>
                                ‚Ä¢ Marketing sees trending products in real-time<br>
                                ‚Ä¢ Data scientists analyze 6 months of purchase patterns<br>
                                ‚Ä¢ All from ONE stream!
                        </div>
                    </div>
                </div>

                <h4>Key Insight: Fan-Out Pattern</h4>
                <p>The same data stream feeds multiple independent consumers simultaneously. This is the power of
                    Kinesis -
                    one ingestion point, many use cases!</p>
            </div>

            <h3>Connection Summary</h3>

            <table>
                <tr>
                    <th>Component</th>
                    <th>Connects To</th>
                    <th>Connection Method</th>
                    <th>Purpose</th>
                </tr>
                <tr>
                    <td><strong>Producers</strong></td>
                    <td>Kinesis Data Streams</td>
                    <td><code>put_record</code> API</td>
                    <td>Send data to stream</td>
                </tr>
                <tr>
                    <td><strong>Kinesis Data Streams</strong></td>
                    <td>Lambda</td>
                    <td>Event source mapping</td>
                    <td>Trigger processing</td>
                </tr>
                <tr>
                    <td><strong>Kinesis Data Streams</strong></td>
                    <td>Kinesis Data Analytics</td>
                    <td>Input stream</td>
                    <td>SQL queries on data</td>
                </tr>
                <tr>
                    <td><strong>Kinesis Data Streams</strong></td>
                    <td>Kinesis Data Firehose</td>
                    <td>Source configuration</td>
                    <td>Automatic delivery</td>
                </tr>
                <tr>
                    <td><strong>Kinesis Data Analytics</strong></td>
                    <td>Kinesis Data Firehose</td>
                    <td>Output stream</td>
                    <td>Send processed results</td>
                </tr>
                <tr>
                    <td><strong>Kinesis Data Firehose</strong></td>
                    <td>S3, Redshift, OpenSearch</td>
                    <td>Destination configuration</td>
                    <td>Store/index data</td>
                </tr>
                <tr>
                    <td><strong>Lambda</strong></td>
                    <td>SNS, DynamoDB, SageMaker</td>
                    <td>AWS SDK (Boto3)</td>
                    <td>Execute actions</td>
                </tr>
            </table>

            <div class="success-box">
                <h3>üí° Design Your Own Pipeline</h3>
                <p><strong>Ask yourself:</strong></p>
                <ol>
                    <li><strong>What's my data source?</strong> (Web app, IoT, logs, etc.)</li>
                    <li><strong>Do I need real-time processing?</strong>
                        <ul>
                            <li>Yes ‚Üí Use Lambda or Kinesis Data Analytics</li>
                            <li>No ‚Üí Just use Firehose to S3</li>
                        </ul>
                    </li>
                    <li><strong>Do I need SQL on the stream?</strong>
                        <ul>
                            <li>Yes ‚Üí Use Kinesis Data Analytics</li>
                            <li>No ‚Üí Use Lambda for custom logic</li>
                        </ul>
                    </li>
                    <li><strong>Where should the data end up?</strong>
                        <ul>
                            <li>Data lake ‚Üí S3 via Firehose</li>
                            <li>Analytics ‚Üí Redshift via Firehose</li>
                            <li>Search ‚Üí OpenSearch via Firehose</li>
                            <li>Alerts ‚Üí SNS via Lambda</li>
                        </ul>
                    </li>
                    <li><strong>Do I need multiple consumers?</strong>
                        <ul>
                            <li>Yes ‚Üí All read from same Kinesis Stream!</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <h2><span class="emoji">üîó</span> How to Implement Kinesis + Firehose Stream</h2>

            <p>This complete guide shows you how to build a production-ready pipeline that streams data from Kinesis
                Data
                Streams to S3 using Kinesis Data Firehose - no consumer code needed!</p>

            <div class="analogy-box">
                <h3>üìã What You'll Build</h3>
                <p><strong>Architecture:</strong> Producer ‚Üí Kinesis Data Stream ‚Üí Kinesis Firehose ‚Üí S3 Bucket</p>
                <p><strong>Use Case:</strong> Automatically archive streaming clickstream data to S3 for later analysis
                    with
                    Athena</p>
                <ul>
                    <li>‚úÖ Kinesis Data Stream collects real-time events</li>
                    <li>‚úÖ Kinesis Firehose batches and delivers to S3</li>
                    <li>‚úÖ Data is compressed (GZIP) and converted to Parquet</li>
                    <li>‚úÖ Partitioned by date for efficient querying</li>
                </ul>
            </div>

            <h3>Step 1: Create Kinesis Data Stream</h3>

            <div class="example-box">
                <h4>Python: Create the Stream</h4>
                <pre><code>import boto3
import time

kinesis = boto3.client('kinesis', region_name='us-east-1')

# Create Kinesis Data Stream
stream_name = 'clickstream-data'

print("Creating Kinesis Data Stream...")
kinesis.create_stream(
    StreamName=stream_name,
    ShardCount=2  # 2 MB/s write, 4 MB/s read capacity
)

# Wait for stream to be active
print("Waiting for stream to become active...")
waiter = kinesis.get_waiter('stream_exists')
waiter.wait(StreamName=stream_name)

print(f"‚úÖ Stream '{stream_name}' is ready!")

# Verify stream
response = kinesis.describe_stream(StreamName=stream_name)
print(f"Status: {response['StreamDescription']['StreamStatus']}")
print(f"Shards: {len(response['StreamDescription']['Shards'])}")</code></pre>
            </div>

            <h3>Step 2: Create S3 Bucket for Firehose Destination</h3>

            <div class="example-box">
                <h4>Python: Create S3 Bucket</h4>
                <pre><code>import boto3

s3 = boto3.client('s3', region_name='us-east-1')

bucket_name = 'my-clickstream-archive'  # Must be globally unique

try:
    # Create bucket
    s3.create_bucket(Bucket=bucket_name)
    print(f"‚úÖ Created S3 bucket: {bucket_name}")
except s3.exceptions.BucketAlreadyOwnedByYou:
    print(f"‚úÖ Bucket {bucket_name} already exists")

# Enable versioning (optional but recommended)
s3.put_bucket_versioning(
    Bucket=bucket_name,
    VersioningConfiguration={'Status': 'Enabled'}
)
print("‚úÖ Versioning enabled")</code></pre>
            </div>

            <h3>Step 3: Create IAM Role for Firehose</h3>

            <div class="example-box">
                <h4>Required IAM Permissions</h4>
                <p>Firehose needs permissions to:</p>
                <ul>
                    <li>Read from Kinesis Data Stream</li>
                    <li>Write to S3 bucket</li>
                    <li>Write logs to CloudWatch</li>
                </ul>

                <h4>IAM Trust Policy (Allow Firehose to assume role)</h4>
                <pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "firehose.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}</code></pre>

                <h4>IAM Permissions Policy</h4>
                <pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "KinesisStreamAccess",
      "Effect": "Allow",
      "Action": [
        "kinesis:DescribeStream",
        "kinesis:GetShardIterator",
        "kinesis:GetRecords",
        "kinesis:ListShards"
      ],
      "Resource": "arn:aws:kinesis:us-east-1:123456789012:stream/clickstream-data"
    },
    {
      "Sid": "S3Access",
      "Effect": "Allow",
      "Action": [
        "s3:PutObject",
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-clickstream-archive",
        "arn:aws:s3:::my-clickstream-archive/*"
      ]
    },
    {
      "Sid": "CloudWatchLogs",
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:us-east-1:123456789012:log-group:/aws/kinesisfirehose/*"
    }
  ]
}</code></pre>

                <h4>Python: Create IAM Role</h4>
                <pre><code>import boto3
import json

iam = boto3.client('iam')

role_name = 'FirehoseDeliveryRole'

# Trust policy
trust_policy = {
    "Version": "2012-10-17",
    "Statement": [{
        "Effect": "Allow",
        "Principal": {"Service": "firehose.amazonaws.com"},
        "Action": "sts:AssumeRole"
    }]
}

# Create role
try:
    role = iam.create_role(
        RoleName=role_name,
        AssumeRolePolicyDocument=json.dumps(trust_policy),
        Description='Role for Kinesis Firehose to deliver data'
    )
    role_arn = role['Role']['Arn']
    print(f"‚úÖ Created role: {role_arn}")
except iam.exceptions.EntityAlreadyExistsException:
    role = iam.get_role(RoleName=role_name)
    role_arn = role['Role']['Arn']
    print(f"‚úÖ Role already exists: {role_arn}")

# Attach permissions policy (use inline policy for this example)
permissions_policy = {
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": ["kinesis:*"],
            "Resource": f"arn:aws:kinesis:us-east-1:123456789012:stream/clickstream-data"
        },
        {
            "Effect": "Allow",
            "Action": ["s3:*"],
            "Resource": [
                f"arn:aws:s3:::my-clickstream-archive",
                f"arn:aws:s3:::my-clickstream-archive/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": ["logs:*"],
            "Resource": "*"
        }
    ]
}

iam.put_role_policy(
    RoleName=role_name,
    PolicyName='FirehosePermissions',
    PolicyDocument=json.dumps(permissions_policy)
)
print("‚úÖ Attached permissions policy")</code></pre>
            </div>

            <h3>Step 4: Create Kinesis Firehose Delivery Stream</h3>

            <div class="example-box">
                <h4>Python: Create Firehose with Kinesis as Source</h4>
                <pre><code>import boto3

firehose = boto3.client('firehose', region_name='us-east-1')

delivery_stream_name = 'clickstream-to-s3'

print("Creating Kinesis Firehose delivery stream...")
response = firehose.create_delivery_stream(
    DeliveryStreamName=delivery_stream_name,
    DeliveryStreamType='KinesisStreamAsSource',  # Read from Kinesis Stream
    
    # Source: Kinesis Data Stream
    KinesisStreamSourceConfiguration={
        'KinesisStreamARN': 'arn:aws:kinesis:us-east-1:123456789012:stream/clickstream-data',
        'RoleARN': 'arn:aws:iam::123456789012:role/FirehoseDeliveryRole'
    },
    
    # Destination: S3
    ExtendedS3DestinationConfiguration={
        'BucketARN': 'arn:aws:s3:::my-clickstream-archive',
        'RoleARN': 'arn:aws:iam::123456789012:role/FirehoseDeliveryRole',
        
        # Partitioning: Organize by date
        'Prefix': 'clickstream/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/',
        'ErrorOutputPrefix': 'errors/',
        
        # Buffering: When to flush to S3
        'BufferingHints': {
            'SizeInMBs': 5,          # Flush when 5 MB accumulated
            'IntervalInSeconds': 300  # Or every 5 minutes (whichever comes first)
        },
        
        # Compression
        'CompressionFormat': 'GZIP',  # Reduce storage costs
        
        # Format conversion: JSON ‚Üí Parquet
        'DataFormatConversionConfiguration': {
            'Enabled': True,
            'SchemaConfiguration': {
                'DatabaseName': 'clickstream_db',
                'TableName': 'events',
                'Region': 'us-east-1',
                'RoleARN': 'arn:aws:iam::123456789012:role/FirehoseDeliveryRole',
                'CatalogId': '123456789012'
            },
            'InputFormatConfiguration': {
                'Deserializer': {
                    'OpenXRecordDeserializer': {
                        'ColumnToJsonKeyMappings': {},
                        'ConvertDotsInJsonKeysToUnderscores': True
                    }
                }
            },
            'OutputFormatConfiguration': {
                'Serializer': {
                    'ParquetSerDe': {
                        'Compression': 'SNAPPY'
                    }
                }
            }
        },
        
        # CloudWatch Logging
        'CloudWatchLoggingOptions': {
            'Enabled': True,
            'LogGroupName': '/aws/kinesisfirehose/clickstream-to-s3',
            'LogStreamName': 'S3Delivery'
        }
    }
)

print(f"‚úÖ Firehose delivery stream created: {delivery_stream_name}")
print(f"ARN: {response['DeliveryStreamARN']}")</code></pre>

                <div class="warning-box">
                    <h4>‚ö†Ô∏è Simplified Version (Without Format Conversion)</h4>
                    <p>If you don't need Parquet conversion, use this simpler configuration:</p>
                    <pre><code>response = firehose.create_delivery_stream(
    DeliveryStreamName='clickstream-to-s3',
    DeliveryStreamType='KinesisStreamAsSource',
    
    KinesisStreamSourceConfiguration={
        'KinesisStreamARN': 'arn:aws:kinesis:us-east-1:123456789012:stream/clickstream-data',
        'RoleARN': 'arn:aws:iam::123456789012:role/FirehoseDeliveryRole'
    },
    
    S3DestinationConfiguration={
        'BucketARN': 'arn:aws:s3:::my-clickstream-archive',
        'RoleARN': 'arn:aws:iam::123456789012:role/FirehoseDeliveryRole',
        'Prefix': 'clickstream/',
        'BufferingHints': {
            'SizeInMBs': 5,
            'IntervalInSeconds': 300
        },
        'CompressionFormat': 'GZIP'
    }
)
print("‚úÖ Simple Firehose created (JSON ‚Üí GZIP ‚Üí S3)")</code></pre>
                </div>
            </div>

            <h3>Step 5: Send Data to Kinesis Stream</h3>

            <div class="example-box">
                <h4>Python: Producer Sending Events</h4>
                <pre><code>import boto3
import json
import time
from datetime import datetime
import random

kinesis = boto3.client('kinesis', region_name='us-east-1')
stream_name = 'clickstream-data'

def send_clickstream_events(num_events=100):
    """Send sample clickstream events"""
    
    users = [f'user-{i:03d}' for i in range(1, 21)]  # 20 users
    pages = ['/home', '/products', '/cart', '/checkout', '/account', '/help']
    actions = ['view', 'click', 'scroll', 'submit', 'search']
    
    print(f"Sending {num_events} events to Kinesis...")
    
    for i in range(num_events):
        event = {
            'event_id': f"evt-{int(time.time() * 1000)}-{i}",
            'user_id': random.choice(users),
            'session_id': f"session-{random.randint(1000, 9999)}",
            'page': random.choice(pages),
            'action': random.choice(actions),
            'timestamp': datetime.utcnow().isoformat(),
            'device': random.choice(['desktop', 'mobile', 'tablet']),
            'browser': random.choice(['chrome', 'firefox', 'safari', 'edge'])
        }
        
        # Send to Kinesis
        response = kinesis.put_record(
            StreamName=stream_name,
            Data=json.dumps(event),
            PartitionKey=event['user_id']
        )
        
        if i % 10 == 0:
            print(f"‚úÖ Sent {i + 1}/{num_events} events")
        
        time.sleep(0.1)  # 10 events/second
    
    print(f"\n‚úÖ All {num_events} events sent to Kinesis!")
    print("Firehose will batch and deliver to S3 within 5 minutes...")

# Run it
send_clickstream_events(100)</code></pre>
            </div>

            <h3>Step 6: Verify Data in S3</h3>

            <div class="example-box">
                <h4>Wait for Firehose to Deliver</h4>
                <p>Firehose batches data based on your buffer settings (5 MB or 5 minutes). Wait at least 5 minutes
                    after
                    sending data.</p>

                <h4>Python: Check S3 for Delivered Files</h4>
                <pre><code>import boto3
from datetime import datetime

s3 = boto3.client('s3')
bucket_name = 'my-clickstream-archive'

# List objects in bucket
print(f"Checking S3 bucket: {bucket_name}\n")

response = s3.list_objects_v2(
    Bucket=bucket_name,
    Prefix='clickstream/'
)

if 'Contents' in response:
    print(f"‚úÖ Found {len(response['Contents'])} files:\n")
    
    for obj in response['Contents']:
        print(f"  üìÑ {obj['Key']}")
        print(f"     Size: {obj['Size']:,} bytes")
        print(f"     Last Modified: {obj['LastModified']}")
        print()
    
    # Download and inspect first file
    if response['Contents']:
        first_file = response['Contents'][0]['Key']
        print(f"\nüì• Downloading {first_file}...")
        
        s3.download_file(
            Bucket=bucket_name,
            Key=first_file,
            Filename='/tmp/sample.gz'
        )
        
        # Decompress and view
        import gzip
        with gzip.open('/tmp/sample.gz', 'rt') as f:
            content = f.read()
            print(f"\nüìÑ File contents (first 500 chars):")
            print(content[:500])
else:
    print("‚ùå No files found yet. Wait a few more minutes for Firehose to deliver.")</code></pre>

                <h4>Expected S3 Structure</h4>
                <pre><code>my-clickstream-archive/
‚îú‚îÄ‚îÄ clickstream/
‚îÇ   ‚îú‚îÄ‚îÄ year=2024/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ month=12/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ day=31/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clickstream-to-s3-1-2024-12-31-04-30-00-abc123.gz
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clickstream-to-s3-1-2024-12-31-04-35-00-def456.gz
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clickstream-to-s3-1-2024-12-31-04-40-00-ghi789.gz
‚îî‚îÄ‚îÄ errors/  (if any delivery failures)</code></pre>
            </div>

            <h3>Step 7: Query Data with Athena (Optional)</h3>

            <div class="example-box">
                <h4>Create Athena Table</h4>
                <pre><code>-- Run in AWS Athena console
CREATE EXTERNAL TABLE clickstream_events (
    event_id STRING,
    user_id STRING,
    session_id STRING,
    page STRING,
    action STRING,
    timestamp STRING,
    device STRING,
    browser STRING
)
PARTITIONED BY (
    year STRING,
    month STRING,
    day STRING
)
STORED AS PARQUET  -- Or INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' for JSON
LOCATION 's3://my-clickstream-archive/clickstream/'
TBLPROPERTIES ('has_encrypted_data'='false');

-- Load partitions
MSCK REPAIR TABLE clickstream_events;

-- Query your data!
SELECT 
    page,
    action,
    COUNT(*) as event_count
FROM clickstream_events
WHERE year = '2024' AND month = '12'
GROUP BY page, action
ORDER BY event_count DESC
LIMIT 10;</code></pre>
            </div>

            <div class="success-box">
                <h3>üéâ Complete Pipeline Summary</h3>
                <p><strong>What You Built:</strong></p>
                <ol>
                    <li>‚úÖ <strong>Kinesis Data Stream</strong> - Ingests real-time events (2 MB/s capacity)</li>
                    <li>‚úÖ <strong>Kinesis Firehose</strong> - Automatically batches data every 5 minutes or 5 MB</li>
                    <li>‚úÖ <strong>S3 Bucket</strong> - Stores compressed, partitioned data</li>
                    <li>‚úÖ <strong>Athena Table</strong> - SQL queries on historical data</li>
                </ol>

                <p><strong>Data Flow:</strong></p>
                <pre><code>Producer (put_record)
  ‚Üì
Kinesis Data Stream (clickstream-data)
  ‚Üì (Firehose polls every second)
Kinesis Firehose (clickstream-to-s3)
  ‚Üì (batches, compresses, partitions)
S3 (my-clickstream-archive/clickstream/year=/month=/day=/)
  ‚Üì
Athena (SQL queries)</code></pre>

                <p><strong>Cost Example (1M events/day, 1 KB each):</strong></p>
                <ul>
                    <li>Kinesis Stream: 2 shards √ó 24h √ó $0.015 = $0.72/day</li>
                    <li>Firehose: 1 GB √ó $0.029 = $0.03/day</li>
                    <li>S3 Storage: 1 GB √ó $0.023 = $0.023/month</li>
                    <li><strong>Total: ~$0.75/day or $23/month</strong></li>
                </ul>
            </div>

            <h3>Common Patterns & Variations</h3>

            <div class="comparison-grid">
                <div class="comparison-column">
                    <h4>Pattern 1: Direct to S3</h4>
                    <p><strong>When:</strong> Just want to archive</p>
                    <pre style="font-size: 0.85em;"><code>Producer
  ‚Üì
Kinesis Stream
  ‚Üì
Firehose ‚Üí S3 (GZIP)</code></pre>
                    <p><strong>Use:</strong> Simple archival, no processing</p>
                </div>

                <div class="comparison-column">
                    <h4>Pattern 2: Transform + S3</h4>
                    <p><strong>When:</strong> Need data enrichment</p>
                    <pre style="font-size: 0.85em;"><code>Producer
  ‚Üì
Kinesis Stream
  ‚Üì
Firehose ‚Üí Lambda
  ‚Üì (transform)
Firehose ‚Üí S3 (Parquet)</code></pre>
                    <p><strong>Use:</strong> Clean, enrich before storing</p>
                </div>

                <div class="comparison-column">
                    <h4>Pattern 3: Multi-Destination</h4>
                    <p><strong>When:</strong> Multiple consumers</p>
                    <pre style="font-size: 0.85em;"><code>Producer
  ‚Üì
Kinesis Stream
  ‚îú‚Üí Firehose ‚Üí S3
  ‚îú‚Üí Lambda (alerts)
  ‚îî‚Üí Analytics (SQL)</code></pre>
                    <p><strong>Use:</strong> Fan-out to many uses</p>
                </div>
            </div>

            <h2><span class="emoji">‚≠ê</span> Features of AWS Kinesis</h2>

            <p>Beyond just streaming data, Kinesis offers key features that make it ideal for production workloads:</p>

            <h3>1. Cost-Efficient (Pay-As-You-Go Model)</h3>

            <div class="concept-box">
                <p><strong>What It Means:</strong> You only pay for what you use - no upfront costs, no minimum fees, no
                    fixed contracts.</p>

                <h4>Pricing Models:</h4>
                <table>
                    <tr>
                        <th>Service</th>
                        <th>Pricing Model</th>
                        <th>Example Cost</th>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Streams</strong></td>
                        <td>Per shard-hour + PUT payload units</td>
                        <td>$0.015/shard-hour + $0.014/million PUT units</td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Analytics</strong></td>
                        <td>Per KPU-hour (Kinesis Processing Unit)</td>
                        <td>$0.11/KPU-hour</td>
                    </tr>
                    <tr>
                        <td><strong>Kinesis Data Firehose</strong></td>
                        <td>Per GB ingested</td>
                        <td>$0.029/GB (first 500 TB/month)</td>
                    </tr>
                </table>

                <h4>Real-World Cost Example:</h4>
                <pre><code># Scenario: Clickstream analytics with 1M events/day
# Each event = 1 KB

# Kinesis Data Streams (2 shards)
Shard hours: 2 shards √ó 24 hours √ó 30 days = 1,440 hours
Cost: 1,440 √ó $0.015 = $21.60/month

Data volume: 1M events/day √ó 30 days √ó 1 KB = 30 GB/month
PUT units: 30 GB = 30,000 PUT units
Cost: 30,000 √ó $0.014/1000 = $0.42/month

Total: $21.60 + $0.42 = ~$22/month for 30M events!

# Kinesis Data Firehose (to S3)
Data ingested: 30 GB/month
Cost: 30 √ó $0.029 = $0.87/month

# Grand Total: ~$23/month for complete real-time pipeline!</code></pre>

                <div class="success-box">
                    <h4>üí∞ Cost Optimization Tips</h4>
                    <ul>
                        <li><strong>Right-size shards:</strong> Monitor metrics, merge underutilized shards</li>
                        <li><strong>Use On-Demand mode:</strong> Auto-scales, no shard management</li>
                        <li><strong>Compress data:</strong> Reduce PUT payload units with GZIP</li>
                        <li><strong>Batch records:</strong> Use <code>put_records</code> instead of multiple
                            <code>put_record</code> calls
                        </li>
                        <li><strong>Adjust retention:</strong> Default 24h is cheaper than 365 days</li>
                    </ul>
                </div>
            </div>

            <h3>2. Seamless Integration with Other AWS Services</h3>

            <div class="concept-box">
                <p><strong>What It Means:</strong> Kinesis works out-of-the-box with dozens of AWS services, allowing
                    you
                    to build complete data pipelines with minimal glue code.</p>

                <h4>Key Integrations:</h4>

                <table>
                    <tr>
                        <th>AWS Service</th>
                        <th>Integration Type</th>
                        <th>Use Case</th>
                    </tr>
                    <tr>
                        <td><strong>AWS Lambda</strong></td>
                        <td>Event source mapping</td>
                        <td>Serverless stream processing, real-time transforms</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon S3</strong></td>
                        <td>Firehose destination</td>
                        <td>Data lake storage, archival, backup</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon Redshift</strong></td>
                        <td>Firehose destination</td>
                        <td>Data warehousing, SQL analytics</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon DynamoDB</strong></td>
                        <td>DynamoDB Streams ‚Üí Kinesis</td>
                        <td>React to database changes in real-time</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon OpenSearch</strong></td>
                        <td>Firehose destination</td>
                        <td>Log analytics, full-text search, dashboards</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon EMR</strong></td>
                        <td>Kinesis connector</td>
                        <td>Big data processing with Spark/Flink</td>
                    </tr>
                    <tr>
                        <td><strong>AWS Glue</strong></td>
                        <td>Schema registry, ETL</td>
                        <td>Data cataloging, transformation pipelines</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon SageMaker</strong></td>
                        <td>Real-time inference</td>
                        <td>ML model predictions on streaming data</td>
                    </tr>
                    <tr>
                        <td><strong>AWS IoT Core</strong></td>
                        <td>IoT Rules ‚Üí Kinesis</td>
                        <td>Device telemetry collection and processing</td>
                    </tr>
                    <tr>
                        <td><strong>Amazon CloudWatch</strong></td>
                        <td>Metrics, alarms, logs</td>
                        <td>Monitoring stream health, alerting</td>
                    </tr>
                </table>

                <h4>Integration Examples:</h4>

                <p><strong>Example 1: DynamoDB ‚Üí Kinesis ‚Üí Lambda</strong></p>
                <pre><code># DynamoDB table with stream enabled
# Every INSERT/UPDATE/DELETE triggers Kinesis event

# Lambda automatically processes changes
def lambda_handler(event, context):
    for record in event['Records']:
        if record['eventName'] == 'INSERT':
            # New item added to DynamoDB
            new_item = record['dynamodb']['NewImage']
            
            # Send notification
            sns.publish(
                TopicArn='arn:aws:sns:...',
                Message=f"New order: {new_item['order_id']}"
            )</code></pre>

                <p><strong>Example 2: IoT Devices ‚Üí Kinesis ‚Üí S3 + OpenSearch</strong></p>
                <pre><code># IoT Rule sends device telemetry to Kinesis
{
  "sql": "SELECT * FROM 'iot/sensors/#'",
  "actions": [{
    "kinesis": {
      "streamName": "iot-telemetry",
      "partitionKey": "${device_id}"
    }
  }]
}

# Firehose splits stream to multiple destinations
# Path 1: S3 for long-term storage
# Path 2: OpenSearch for real-time dashboards</code></pre>

                <p><strong>Example 3: Kinesis ‚Üí Lambda ‚Üí SageMaker (Real-Time ML)</strong></p>
                <pre><code># Stream: Financial transactions
# Lambda: Sends to SageMaker for fraud detection

import boto3
sagemaker = boto3.client('sagemaker-runtime')

def lambda_handler(event, context):
    for record in event['Records']:
        transaction = json.loads(base64.b64decode(record['kinesis']['data']))
        
        # Get fraud score from SageMaker endpoint
        response = sagemaker.invoke_endpoint(
            EndpointName='fraud-detection-model',
            Body=json.dumps(transaction)
        )
        
        prediction = json.loads(response['Body'].read())
        
        if prediction['fraud_score'] > 0.8:
            # Block transaction, alert fraud team
            block_transaction(transaction['id'])
            send_alert(transaction)</code></pre>
            </div>

            <h3>3. High Availability</h3>

            <div class="concept-box">
                <p><strong>What It Means:</strong> Access your data streams from anywhere, anytime. Kinesis is built on
                    AWS's global infrastructure with automatic redundancy.</p>

                <h4>Availability Features:</h4>
                <ul>
                    <li><strong>Multi-AZ Replication:</strong> Data automatically replicated across 3 Availability Zones
                    </li>
                    <li><strong>99.9% SLA:</strong> Guaranteed uptime for Data Streams</li>
                    <li><strong>Global Access:</strong> Access from any AWS region, on-premises, or edge locations</li>
                    <li><strong>Automatic Failover:</strong> If one AZ fails, traffic seamlessly routes to healthy AZs
                    </li>
                    <li><strong>No Downtime Scaling:</strong> Add/remove shards without stopping stream</li>
                </ul>

                <h4>Access Patterns:</h4>
                <table>
                    <tr>
                        <th>Access From</th>
                        <th>Method</th>
                        <th>Example</th>
                    </tr>
                    <tr>
                        <td>Web/Mobile App</td>
                        <td>AWS SDK (JavaScript, iOS, Android)</td>
                        <td>Track user events directly from browser/phone</td>
                    </tr>
                    <tr>
                        <td>On-Premises Servers</td>
                        <td>Kinesis Agent, AWS SDK</td>
                        <td>Stream logs from data center to AWS</td>
                    </tr>
                    <tr>
                        <td>IoT Devices</td>
                        <td>AWS IoT Core ‚Üí Kinesis</td>
                        <td>Smart home sensors sending telemetry</td>
                    </tr>
                    <tr>
                        <td>Third-Party Services</td>
                        <td>HTTP endpoint (Firehose)</td>
                        <td>Datadog, Splunk ingesting data</td>
                    </tr>
                    <tr>
                        <td>AWS Services</td>
                        <td>Native integrations</td>
                        <td>Lambda, EC2, ECS automatically consuming</td>
                    </tr>
                </table>

                <h4>Disaster Recovery:</h4>
                <pre><code># Example: Multi-region setup for critical streams

# Primary region: us-east-1
primary_stream = kinesis.create_stream(
    StreamName='critical-events',
    ShardCount=5
)

# Backup region: us-west-2 (replicate with Lambda)
def replicate_to_backup(event, context):
    """Lambda in us-east-1 replicates to us-west-2"""
    kinesis_backup = boto3.client('kinesis', region_name='us-west-2')
    
    for record in event['Records']:
        kinesis_backup.put_record(
            StreamName='critical-events-backup',
            Data=record['kinesis']['data'],
            PartitionKey=record['kinesis']['partitionKey']
        )
# Now you have redundancy across regions!</code></pre>

                <div class="success-box">
                    <h4>üåç Global Availability Benefits</h4>
                    <ul>
                        <li><strong>24/7 Operation:</strong> No maintenance windows, always on</li>
                        <li><strong>Remote Teams:</strong> Developers worldwide can access same stream</li>
                        <li><strong>Edge Computing:</strong> AWS Outposts can send data to Kinesis</li>
                        <li><strong>Cross-Region Analytics:</strong> Combine streams from multiple regions</li>
                    </ul>
                </div>
            </div>

            <h3>4. Real-Time Processing</h3>

            <div class="concept-box">
                <p><strong>What It Means:</strong> Process data as it arrives - no waiting for batch jobs. Make
                    decisions
                    in milliseconds, not hours.</p>

                <h4>Real-Time Capabilities:</h4>
                <table>
                    <tr>
                        <th>Capability</th>
                        <th>Latency</th>
                        <th>Use Case</th>
                    </tr>
                    <tr>
                        <td><strong>Data Ingestion</strong></td>
                        <td>
                            < 1 second</td>
                        <td>Click events appear in stream immediately</td>
                    </tr>
                    <tr>
                        <td><strong>Lambda Processing</strong></td>
                        <td>
                            < 1 second</td>
                        <td>Trigger actions on incoming data</td>
                    </tr>
                    <tr>
                        <td><strong>Analytics Queries</strong></td>
                        <td>
                            < 1 second</td>
                        <td>SQL aggregations on live stream</td>
                    </tr>
                    <tr>
                        <td><strong>Firehose Delivery</strong></td>
                        <td>60-900 seconds</td>
                        <td>Batched delivery to S3/Redshift</td>
                    </tr>
                </table>

                <h4>Real-Time vs. Batch Processing:</h4>
                <div class="comparison-grid">
                    <div class="comparison-column">
                        <h4>‚ö° Real-Time (Kinesis)</h4>
                        <ul>
                            <li><strong>Latency:</strong> Milliseconds to seconds</li>
                            <li><strong>Use:</strong> Fraud detection, live dashboards</li>
                            <li><strong>Example:</strong> Block fraudulent transaction as it happens</li>
                        </ul>
                    </div>

                    <div class="comparison-column">
                        <h4>‚è∞ Batch (Traditional)</h4>
                        <ul>
                            <li><strong>Latency:</strong> Hours to days</li>
                            <li><strong>Use:</strong> Historical reporting, ETL</li>
                            <li><strong>Example:</strong> Generate yesterday's sales report</li>
                        </ul>
                    </div>

                    <div class="comparison-column">
                        <h4>üéØ Best of Both</h4>
                        <ul>
                            <li><strong>Latency:</strong> Real-time + batch</li>
                            <li><strong>Use:</strong> Lambda Kappa architecture</li>
                            <li><strong>Example:</strong> Live alerts + historical analysis</li>
                        </ul>
                    </div>
                </div>

                <h4>Real-Time Processing Examples:</h4>

                <p><strong>Example 1: Live Gaming Leaderboard</strong></p>
                <pre><code># Players send scores to Kinesis in real-time
def submit_score(player_id, score):
    kinesis.put_record(
        StreamName='game-scores',
        Data=json.dumps({'player': player_id, 'score': score}),
        PartitionKey=player_id
    )

# Kinesis Analytics updates leaderboard every second
-- SQL: Top 10 players in last minute
SELECT player, 
       MAX(score) as high_score
FROM game_scores
GROUP BY player, STEP(ROWTIME BY INTERVAL '1' MINUTE)
ORDER BY high_score DESC
LIMIT 10;

# Leaderboard updates in < 1 second!</code></pre>

                <p><strong>Example 2: Stock Trading Alerts</strong></p>
                <pre><code># Stock prices stream in real-time
# Lambda triggers alert if price drops > 5%

def lambda_handler(event, context):
    for record in event['Records']:
        stock = json.loads(base64.b64decode(record['kinesis']['data']))
        
        # Real-time calculation
        price_change = (stock['current'] - stock['previous']) / stock['previous']
        
        if price_change < -0.05:  # 5% drop
            # IMMEDIATE alert (< 100ms from event)
            sns.publish(
                TopicArn='arn:aws:sns:...',
                Subject=f"üö® {stock['symbol']} dropped 5%!",
                Message=f"Current: ${stock['current']}"
            )
            
# Traditional batch: You'd learn about the drop tomorrow!
# Kinesis: You know in milliseconds</code></pre>

                <p><strong>Example 3: IoT Anomaly Detection</strong></p>
                <pre><code># Temperature sensors send data every 10 seconds
# Kinesis Analytics detects anomalies in real-time

-- SQL: Alert if temperature > 80¬∞F for > 5 minutes
SELECT device_id,
       AVG(temperature) as avg_temp,
       MAX(temperature) as max_temp
FROM sensor_stream
GROUP BY device_id, STEP(ROWTIME BY INTERVAL '5' MINUTE)
HAVING MAX(temperature) > 80;

# If anomaly detected ‚Üí Lambda ‚Üí SNS ‚Üí Maintenance team paged
# Response time: < 5 minutes vs. next day batch processing</code></pre>

                <div class="success-box">
                    <h4>‚ö° Real-Time Processing Benefits</h4>
                    <ul>
                        <li><strong>Instant Insights:</strong> See trends as they happen, not yesterday's news</li>
                        <li><strong>Faster Response:</strong> Block fraud, reroute traffic, alert teams immediately</li>
                        <li><strong>Better UX:</strong> Real-time recommendations, live dashboards</li>
                        <li><strong>Competitive Edge:</strong> React to market changes before competitors</li>
                    </ul>
                </div>
            </div>

            <h2><span class="emoji">üìä</span> Kinesis Technical Specifications</h2>

            <div class="spec-box">
                <h3>Key Limits You Must Know</h3>
                <table>
                    <tr>
                        <th>Specification</th>
                        <th>Value</th>
                        <th>Why It Matters</th>
                    </tr>
                    <tr>
                        <td><strong>Shard Write Capacity</strong></td>
                        <td><code>1 MB/s or 1,000 records/s</code></td>
                        <td>Exceeding this = throttling errors</td>
                    </tr>
                    <tr>
                        <td><strong>Shard Read Capacity</strong></td>
                        <td><code>2 MB/s</code></td>
                        <td>Limit per consumer (use enhanced fan-out for more)</td>
                    </tr>
                    <tr>
                        <td><strong>Record Size</strong></td>
                        <td><code>1 MB max</code></td>
                        <td>Large records = fewer records per shard</td>
                    </tr>
                    <tr>
                        <td><strong>Data Retention</strong></td>
                        <td><code>24 hours (default) ‚Üí 365 days (max)</code></td>
                        <td>How long data stays in stream</td>
                    </tr>
                    <tr>
                        <td><strong>Partition Key</strong></td>
                        <td><code>256 bytes max</code></td>
                        <td>Used for shard routing</td>
                    </tr>
                </table>
            </div>

            <h2><span class="emoji">üÜö</span> Kinesis vs. SQS vs. Kafka</h2>

            <div class="comparison-grid">
                <div class="comparison-column">
                    <h3>üåä Kinesis</h3>
                    <ul>
                        <li><strong>Model:</strong> Real-time stream</li>
                        <li><strong>Use:</strong> High-throughput streaming</li>
                        <li><strong>Retention:</strong> 24h - 365 days</li>
                        <li><strong>Consumers:</strong> Multiple can read same data</li>
                        <li><strong>Ordering:</strong> Per shard (partition key)</li>
                        <li><strong>Example:</strong> Clickstream analytics, IoT telemetry</li>
                    </ul>
                </div>

                <div class="comparison-column">
                    <h3>üì¨ SQS</h3>
                    <ul>
                        <li><strong>Model:</strong> Message queue</li>
                        <li><strong>Use:</strong> Decoupling services</li>
                        <li><strong>Retention:</strong> 1 min - 14 days</li>
                        <li><strong>Consumers:</strong> One consumes, message deleted</li>
                        <li><strong>Ordering:</strong> Best-effort (FIFO available)</li>
                        <li><strong>Example:</strong> Order processing, job queue</li>
                    </ul>
                </div>

                <div class="comparison-column">
                    <h3>üéØ Kafka</h3>
                    <ul>
                        <li><strong>Model:</strong> Distributed log</li>
                        <li><strong>Use:</strong> High-throughput, self-managed</li>
                        <li><strong>Retention:</strong> Configurable (days/weeks)</li>
                        <li><strong>Consumers:</strong> Multiple consumer groups</li>
                        <li><strong>Ordering:</strong> Per partition</li>
                        <li><strong>Example:</strong> On-prem systems, custom needs</li>
                    </ul>
                </div>
            </div>

            <h3>When to Use What</h3>
            <table>
                <tr>
                    <th>Scenario</th>
                    <th>Use</th>
                    <th>Why</th>
                </tr>
                <tr>
                    <td>Process 1M events/second in real-time</td>
                    <td><strong>Kinesis</strong></td>
                    <td>Built for high-throughput streaming</td>
                </tr>
                <tr>
                    <td>Decouple microservices (job queue)</td>
                    <td><strong>SQS</strong></td>
                    <td>Simpler, message consumed once</td>
                </tr>
                <tr>
                    <td>Multiple teams need same data stream</td>
                    <td><strong>Kinesis</strong></td>
                    <td>Fan-out to multiple consumers</td>
                </tr>
                <tr>
                    <td>Need exactly-once semantics</td>
                    <td><strong>Kafka (MSK)</strong></td>
                    <td>More control over guarantees</td>
                </tr>
                <tr>
                    <td>Simple async task processing</td>
                    <td><strong>SQS</strong></td>
                    <td>Lower cost, easier setup</td>
                </tr>
            </table>

            <h2><span class="emoji">üõ†Ô∏è</span> Hands-On Tutorial: Create and Use AWS Kinesis Data Stream</h2>

            <p>This complete tutorial will walk you through creating your first Kinesis Data Stream, sending data to it,
                and
                consuming that data - step by step!</p>

            <div class="analogy-box">
                <h3>üìã What You'll Build</h3>
                <p>By the end of this tutorial, you'll have:</p>
                <ul>
                    <li>‚úÖ A working Kinesis Data Stream with 2 shards</li>
                    <li>‚úÖ A Python producer sending clickstream events</li>
                    <li>‚úÖ A Lambda consumer processing events in real-time</li>
                    <li>‚úÖ CloudWatch monitoring showing throughput metrics</li>
                </ul>
                <p><strong>Time Required:</strong> ~30 minutes</p>
            </div>

            <h3>Step 1: Create Your First Kinesis Data Stream</h3>

            <div class="example-box">
                <h4>Option A: Using AWS Console (Recommended for Beginners)</h4>
                <ol>
                    <li><strong>Navigate to Kinesis:</strong>
                        <ul>
                            <li>Log in to AWS Console</li>
                            <li>Search for "Kinesis" in the search bar</li>
                            <li>Click on "Amazon Kinesis"</li>
                        </ul>
                    </li>
                    <li><strong>Create Data Stream:</strong>
                        <ul>
                            <li>Click "Data streams" in the left sidebar</li>
                            <li>Click "Create data stream" button</li>
                        </ul>
                    </li>
                    <li><strong>Configure Stream:</strong>
                        <ul>
                            <li><strong>Data stream name:</strong> <code>my-first-stream</code></li>
                            <li><strong>Capacity mode:</strong> Select "Provisioned"</li>
                            <li><strong>Provisioned shards:</strong> Enter <code>2</code></li>
                        </ul>
                    </li>
                    <li><strong>Create:</strong>
                        <ul>
                            <li>Click "Create data stream"</li>
                            <li>Wait ~60 seconds for status to show "Active"</li>
                        </ul>
                    </li>
                </ol>

                <div class="success-box">
                    <h4>‚úÖ Verification</h4>
                    <p>You should see:</p>
                    <ul>
                        <li>Stream name: <code>my-first-stream</code></li>
                        <li>Status: <span style="color: #10b981;">‚óè Active</span></li>
                        <li>Number of open shards: 2</li>
                        <li>Retention period: 24 hours</li>
                    </ul>
                </div>
            </div>

            <div class="example-box">
                <h4>Option B: Using Python (Boto3)</h4>
                <pre><code>import boto3
import time

# Create Kinesis client
kinesis = boto3.client('kinesis', region_name='us-east-1')

# Create the stream
print("Creating stream 'my-first-stream'...")
response = kinesis.create_stream(
    StreamName='my-first-stream',
    ShardCount=2
)

print("Waiting for stream to become active...")
# Wait for stream to be ready
while True:
    response = kinesis.describe_stream(StreamName='my-first-stream')
    status = response['StreamDescription']['StreamStatus']
    
    print(f"Current status: {status}")
    
    if status == 'ACTIVE':
        print("‚úÖ Stream is ready!")
        break
    elif status in ['CREATING', 'UPDATING']:
        time.sleep(5)  # Wait 5 seconds before checking again
    else:
        print(f"‚ùå Unexpected status: {status}")
        break

# Show stream details
stream_info = response['StreamDescription']
print(f"\nüìä Stream Details:")
print(f"  Name: {stream_info['StreamName']}")
print(f"  Status: {stream_info['StreamStatus']}")
print(f"  Shards: {len(stream_info['Shards'])}")
print(f"  Retention: {stream_info['RetentionPeriodHours']} hours")</code></pre>
            </div>

            <h3>Step 2: Set Up IAM Permissions (Required for Production)</h3>

            <div class="example-box">
                <h4>Create IAM Role for Kinesis Access</h4>
                <p><strong>For Lambda Consumer:</strong> Create role with this policy</p>
                <pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kinesis:GetRecords",
        "kinesis:GetShardIterator",
        "kinesis:DescribeStream",
        "kinesis:ListShards",
        "kinesis:ListStreams"
      ],
      "Resource": "arn:aws:kinesis:us-east-1:*:stream/my-first-stream"
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents"
      ],
      "Resource": "arn:aws:logs:*:*:*"
    }
  ]
}</code></pre>

                <p><strong>For Producer Application:</strong></p>
                <pre><code>{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "kinesis:PutRecord",
        "kinesis:PutRecords",
        "kinesis:DescribeStream"
      ],
      "Resource": "arn:aws:kinesis:us-east-1:*:stream/my-first-stream"
    }
  ]
}</code></pre>
            </div>

            <h3>Step 3: Send Data to Your Stream (Producer)</h3>

            <div class="example-box">
                <h4>Create a Python Producer</h4>
                <p>Save this as <code>producer.py</code>:</p>
                <pre><code>import boto3
import json
import time
from datetime import datetime
import random

kinesis = boto3.client('kinesis', region_name='us-east-1')
stream_name = 'my-first-stream'

def generate_clickstream_event():
    """Generate fake clickstream data"""
    users = ['user-001', 'user-002', 'user-003', 'user-004', 'user-005']
    pages = ['/home', '/products', '/cart', '/checkout', '/account']
    actions = ['view', 'click', 'scroll', 'submit']
    
    return {
        'user_id': random.choice(users),
        'page': random.choice(pages),
        'action': random.choice(actions),
        'timestamp': datetime.utcnow().isoformat(),
        'session_id': f"session-{random.randint(1000, 9999)}"
    }

def send_events(num_events=10):
    """Send events to Kinesis"""
    print(f"Sending {num_events} events to stream '{stream_name}'...\n")
    
    for i in range(num_events):
        event = generate_clickstream_event()
        
        # Send to Kinesis
        response = kinesis.put_record(
            StreamName=stream_name,
            Data=json.dumps(event),
            PartitionKey=event['user_id']  # Same user goes to same shard
        )
        
        print(f"‚úÖ Event {i+1}: User {event['user_id']} ‚Üí {event['action']} on {event['page']}")
        print(f"   Shard: {response['ShardId']}, Sequence: {response['SequenceNumber'][:20]}...")
        
        time.sleep(0.5)  # Wait 0.5 seconds between events

if __name__ == "__main__":
    send_events(10)
    print("\n‚úÖ All events sent successfully!")</code></pre>

                <h4>Run the Producer</h4>
                <pre><code>python producer.py</code></pre>

                <h4>Expected Output:</h4>
                <pre><code>Sending 10 events to stream 'my-first-stream'...

‚úÖ Event 1: User user-003 ‚Üí view on /home
   Shard: shardId-000000000000, Sequence: 495648391283746261...
‚úÖ Event 2: User user-001 ‚Üí click on /products
   Shard: shardId-000000000001, Sequence: 495648391283835642...
...
‚úÖ All events sent successfully!</code></pre>
            </div>

            <h3>Step 4: Consume Data from Your Stream (Consumer)</h3>

            <div class="example-box">
                <h4>Option A: Create Lambda Consumer (Recommended)</h4>

                <p><strong>Lambda Function Code:</strong> Save as <code>lambda_function.py</code></p>
                <pre><code>import json
import base64

def lambda_handler(event, context):
    """Process records from Kinesis stream"""
    
    print(f"Received {len(event['Records'])} records")
    
    for record in event['Records']:
        # Decode the data
        payload = base64.b64decode(record['kinesis']['data'])
        data = json.loads(payload)
        
        # Process the event
        print(f"Processing event:")
        print(f"  User: {data['user_id']}")
        print(f"  Page: {data['page']}")
        print(f"  Action: {data['action']}")
        print(f"  Time: {data['timestamp']}")
        
        # Your processing logic here
        # Example: Count page views, detect fraud, send alerts, etc.
    
    return {
        'statusCode': 200,
        'body': json.dumps(f'Processed {len(event["Records"])} records')
    }</code></pre>

                <p><strong>Deploy Lambda Function:</strong></p>
                <ol>
                    <li>Go to AWS Lambda console</li>
                    <li>Click "Create function"</li>
                    <li>Function name: <code>kinesis-processor</code></li>
                    <li>Runtime: Python 3.12</li>
                    <li>Paste the code above</li>
                    <li>Click "Create function"</li>
                </ol>

                <p><strong>Connect Kinesis to Lambda:</strong></p>
                <ol>
                    <li>In Lambda function page, click "Add trigger"</li>
                    <li>Select "Kinesis"</li>
                    <li>Kinesis stream: <code>my-first-stream</code></li>
                    <li>Starting position: "Latest"</li>
                    <li>Batch size: 100</li>
                    <li>Click "Add"</li>
                </ol>
            </div>

            <div class="example-box">
                <h4>Option B: Python Consumer Script</h4>
                <p>Save this as <code>consumer.py</code>:</p>
                <pre><code>import boto3
import json
import time

kinesis = boto3.client('kinesis', region_name='us-east-1')
stream_name = 'my-first-stream'

# Get shard information
response = kinesis.describe_stream(StreamName=stream_name)
shard_id = response['StreamDescription']['Shards'][0]['ShardId']

print(f"Reading from shard: {shard_id}\n")

# Get shard iterator (starting from latest records)
iterator_response = kinesis.get_shard_iterator(
    StreamName=stream_name,
    ShardId=shard_id,
    ShardIteratorType='LATEST'  # Or 'TRIM_HORIZON' for oldest
)

shard_iterator = iterator_response['ShardIterator']

print("Listening for events... (Press Ctrl+C to stop)\n")

try:
    record_count = 0
    while True:
        # Get records from the shard
        response = kinesis.get_records(
            ShardIterator=shard_iterator,
            Limit=100
        )
        
        records = response['Records']
        
        if records:
            for record in records:
                data = json.loads(record['Data'])
                record_count += 1
                
                print(f"üì¶ Record {record_count}:")
                print(f"   User: {data['user_id']}")
                print(f"   Action: {data['action']} on {data['page']}")
                print(f"   Time: {data['timestamp']}\n")
        
        # Get next iterator
        shard_iterator = response['NextShardIterator']
        
        # Sleep to avoid hitting API limits
        time.sleep(1)

except KeyboardInterrupt:
    print(f"\n‚úÖ Stopped. Processed {record_count} records.")</code></pre>

                <h4>Run the Consumer (in another terminal):</h4>
                <pre><code>python consumer.py</code></pre>

                <p>Now run the producer again, and you'll see records appear in the consumer!</p>
            </div>

            <h3>Step 5: Monitor Your Stream</h3>

            <div class="example-box">
                <h4>View Metrics in CloudWatch</h4>
                <ol>
                    <li>Go to AWS CloudWatch console</li>
                    <li>Click "Metrics" ‚Üí "All metrics"</li>
                    <li>Select "Kinesis" ‚Üí "Stream Metrics"</li>
                    <li>Select your stream: <code>my-first-stream</code></li>
                    <li>Add these metrics to graph:
                        <ul>
                            <li><code>IncomingRecords</code> - Records written per second</li>
                            <li><code>IncomingBytes</code> - Data throughput</li>
                            <li><code>GetRecords.Records</code> - Records read</li>
                            <li><code>WriteProvisionedThroughputExceeded</code> - Throttling errors</li>
                        </ul>
                    </li>
                </ol>

                <h4>Check Metrics with Python:</h4>
                <pre><code>import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')

# Get metrics for last hour
end_time = datetime.utcnow()
start_time = end_time - timedelta(hours=1)

response = cloudwatch.get_metric_statistics(
    Namespace='AWS/Kinesis',
    MetricName='IncomingRecords',
    Dimensions=[
        {'Name': 'StreamName', 'Value': 'my-first-stream'}
    ],
    StartTime=start_time,
    EndTime=end_time,
    Period=300,  # 5-minute intervals
    Statistics=['Sum']
)

print("üìä Incoming Records (last hour):")
for datapoint in sorted(response['Datapoints'], key=lambda x: x['Timestamp']):
    print(f"  {datapoint['Timestamp']}: {int(datapoint['Sum'])} records")</code></pre>
            </div>

            <h3>Step 6: Clean Up Resources (Important!)</h3>

            <div class="warning-box">
                <h4>‚ö†Ô∏è Avoid Unexpected Charges</h4>
                <p>Kinesis streams cost $0.015 per shard-hour. Remember to delete streams you're not using!</p>
            </div>

            <div class="example-box">
                <h4>Delete Stream via Console:</h4>
                <ol>
                    <li>Go to Kinesis console</li>
                    <li>Select your stream: <code>my-first-stream</code></li>
                    <li>Click "Actions" ‚Üí "Delete"</li>
                    <li>Type the stream name to confirm</li>
                    <li>Click "Delete"</li>
                </ol>

                <h4>Delete Stream via Python:</h4>
                <pre><code>import boto3

kinesis = boto3.client('kinesis', region_name='us-east-1')

# Delete the stream
print("Deleting stream 'my-first-stream'...")
kinesis.delete_stream(
    StreamName='my-first-stream',
    EnforceConsumerDeletion=True  # Delete consumers too
)

print("‚úÖ Stream deletion initiated (will complete in ~5 minutes)")</code></pre>

                <h4>Verify Deletion:</h4>
                <pre><code># List all streams
response = kinesis.list_streams()
print("Active streams:", response['StreamNames'])

# 'my-first-stream' should not be in the list</code></pre>
            </div>

            <div class="success-box">
                <h3>üéâ Congratulations!</h3>
                <p>You've successfully:</p>
                <ul>
                    <li>‚úÖ Created a Kinesis Data Stream</li>
                    <li>‚úÖ Sent data to the stream with a Python producer</li>
                    <li>‚úÖ Consumed data with Lambda or Python consumer</li>
                    <li>‚úÖ Monitored stream metrics in CloudWatch</li>
                    <li>‚úÖ Cleaned up resources</li>
                </ul>
                <p><strong>Next Steps:</strong></p>
                <ul>
                    <li>Try integrating with Kinesis Data Analytics for SQL queries</li>
                    <li>Set up Kinesis Data Firehose to archive data to S3</li>
                    <li>Build a real-time dashboard with the data</li>
                    <li>Implement error handling and retry logic</li>
                </ul>
            </div>

            <h2><span class="emoji">üõ†Ô∏è</span> Creating and Using Kinesis Streams</h2>

            <h3>Method 1: AWS Console</h3>

            <div class="example-box">
                <h4>Step 1: Create Stream</h4>
                <ol>
                    <li>Go to <strong>AWS Console</strong> ‚Üí Search "Kinesis"</li>
                    <li>Click <strong>"Create data stream"</strong></li>
                    <li>Name: <code>website-clickstream</code></li>
                    <li>Capacity mode:
                        <ul>
                            <li><strong>On-demand:</strong> Auto-scales (pay per GB)</li>
                            <li><strong>Provisioned:</strong> Set shard count (pay per shard-hour)</li>
                        </ul>
                    </li>
                    <li>For provisioned, set shards: <code>2</code> (start small)</li>
                    <li>Click <strong>"Create data stream"</strong></li>
                </ol>
            </div>

            <h3>Method 2: Python (Boto3)</h3>

            <div class="example-box">
                <h4>Create Stream</h4>
                <pre><code>import boto3

kinesis = boto3.client('kinesis', region_name='us-east-1')

# Create stream with 2 shards
kinesis.create_stream(
    StreamName='website-clickstream',
    ShardCount=2
)

print("Stream created. Wait ~60 seconds for ACTIVE status...")

# Wait for stream to be ready
import time
while True:
    response = kinesis.describe_stream(StreamName='website-clickstream')
    status = response['StreamDescription']['StreamStatus']
    print(f"Status: {status}")
    
    if status == 'ACTIVE':
        print("Stream is ready!")
        break
    
    time.sleep(10)</code></pre>
            </div>

            <div class="example-box">
                <h4>Complete Example with Authentication</h4>
                <pre><code>import boto3
import json

# Method 1: Using AWS credentials explicitly
session = boto3.Session(
    aws_access_key_id='YOUR_ACCESS_KEY',
    aws_secret_access_key='YOUR_SECRET_KEY',
    region_name='us-east-1'
)

# Create a Kinesis client
kinesis_client = session.client('kinesis')

# Define the name of your Kinesis data stream
stream_name = 'de_academy_stream'

# Sample data to put into the stream (can be a dictionary or any JSON-serializable object)
sample_data = {
    'example_key': 'example_value',
    'timestamp': '2024-06-26T06:17:38Z',
    'user_id': 'user-123'
}

# Convert sample_data to JSON string
data = json.dumps(sample_data)

# Put a record into the Kinesis stream
response = kinesis_client.put_record(
    StreamName=stream_name,
    Data=data,
    PartitionKey='1'  # Determines which shard the record goes to
)

# Print the response (contains metadata about the record put operation)
print(response)</code></pre>

                <h4>Response Output:</h4>
                <pre><code>{
    'ShardId': 'shardId-000000000003',
    'SequenceNumber': '49653353468219810426419072694234682215307642087422296114',
    'ResponseMetadata': {
        'RequestId': 'd4218499-45ab-af43-8ad4-305bd88fdff4',
        'HTTPStatusCode': 200,
        'HTTPHeaders': {
            'x-amzn-requestid': 'd4218499-45ab-af43-8ad4-305bd88fdff4',
            'x-amz-id-2': 'lyi3FB/bH/Wzyi9Z6ZL1r9+8/MQun2h/Xw1x6qmwdwG5jTAT0iQksOKwwbIrSnmWNq+m8aLU9AM5gzZDSys6whqf2pSxyH1I',
            'date': 'Wed, 26 Jun 2024 06:17:38 GMT',
            'content-type': 'application/x-amz-json-1.1',
            'content-length': '110',
            'connection': 'keep-alive'
        },
        'RetryAttempts': 0
    }
}</code></pre>

                <h4>Understanding the Response:</h4>
                <table>
                    <tr>
                        <th>Field</th>
                        <th>Description</th>
                        <th>Example Value</th>
                    </tr>
                    <tr>
                        <td><code>ShardId</code></td>
                        <td>Which shard received the record</td>
                        <td>shardId-000000000003</td>
                    </tr>
                    <tr>
                        <td><code>SequenceNumber</code></td>
                        <td>Unique identifier for this record in the shard (ordered)</td>
                        <td>49653353468219810426419072694234...</td>
                    </tr>
                    <tr>
                        <td><code>HTTPStatusCode</code></td>
                        <td>HTTP status (200 = success)</td>
                        <td>200</td>
                    </tr>
                    <tr>
                        <td><code>RequestId</code></td>
                        <td>Unique ID for this API call (for debugging)</td>
                        <td>d4218499-45ab-af43...</td>
                    </tr>
                </table>

                <div class="success-box">
                    <h4>üí° Best Practices for Authentication</h4>
                    <ul>
                        <li><strong>Don't hardcode credentials!</strong> Use environment variables or IAM roles instead
                        </li>
                        <li><strong>Better approach:</strong> Configure AWS CLI with <code>aws configure</code></li>
                        <li><strong>Best approach:</strong> Use IAM roles when running on EC2/Lambda/ECS</li>
                    </ul>

                    <h4>Recommended Authentication Method:</h4>
                    <pre><code># Method 2: Using AWS CLI configuration (recommended)
# Run: aws configure
# Then use:
kinesis = boto3.client('kinesis')  # Automatically uses configured credentials

# Method 3: Using IAM roles (best for production)
# If running on EC2/Lambda/ECS, credentials are automatic!
kinesis = boto3.client('kinesis', region_name='us-east-1')</code></pre>
                </div>
            </div>

            <div class="example-box">
                <h4>Put Records (Producer)</h4>
                <pre><code>import boto3
import json
import time
from datetime import datetime

kinesis = boto3.client('kinesis')

def send_click_event(user_id, page, action):
    """Send a clickstream event to Kinesis"""
    record = {
        'user_id': user_id,
        'page': page,
        'action': action,
        'timestamp': datetime.utcnow().isoformat()
    }
    
    # Put record to stream
    response = kinesis.put_record(
        StreamName='website-clickstream',
        Data=json.dumps(record),
        PartitionKey=user_id  # Route by user_id for even distribution
    )
    
    print(f"Record sent to shard: {response['ShardId']}")
    print(f"Sequence number: {response['SequenceNumber']}")
    return response

# Send some sample events
send_click_event('user-123', '/home', 'view')
send_click_event('user-456', '/product/789', 'click')
send_click_event('user-123', '/cart', 'add_to_cart')
send_click_event('user-789', '/checkout', 'purchase')</code></pre>
            </div>

            <div class="example-box">
                <h4>Batch Put Records (More Efficient)</h4>
                <pre><code>import boto3
import json

kinesis = boto3.client('kinesis')

def send_batch_events(events):
    """Send multiple records in one API call (more efficient)"""
    records = []
    
    for event in events:
        records.append({
            'Data': json.dumps(event),
            'PartitionKey': event['user_id']
        })
    
    # Send up to 500 records at once
    response = kinesis.put_records(
        StreamName='website-clickstream',
        Records=records
    )
    
    # Check for failures
    failed_count = response['FailedRecordCount']
    if failed_count > 0:
        print(f"‚ö†Ô∏è {failed_count} records failed!")
        for i, record in enumerate(response['Records']):
            if 'ErrorCode' in record:
                print(f"Record {i} failed: {record['ErrorCode']}")
    else:
        print(f"‚úÖ All {len(records)} records sent successfully!")

# Send batch
events = [
    {'user_id': 'user-1', 'page': '/home', 'action': 'view'},
    {'user_id': 'user-2', 'page': '/product/1', 'action': 'click'},
    {'user_id': 'user-3', 'page': '/cart', 'action': 'add'},
]

send_batch_events(events)</code></pre>
            </div>

            <div class="example-box">
                <h4>Get Records (Consumer)</h4>
                <pre><code>import boto3
import json
import time

kinesis = boto3.client('kinesis')

# Step 1: Get shard iterator
stream_name = 'website-clickstream'

# List shards
response = kinesis.describe_stream(StreamName=stream_name)
shard_id = response['StreamDescription']['Shards'][0]['ShardId']

# Get iterator to start reading from latest
shard_iterator_response = kinesis.get_shard_iterator(
    StreamName=stream_name,
    ShardId=shard_id,
    ShardIteratorType='LATEST'  # Or 'TRIM_HORIZON' for oldest
)

shard_iterator = shard_iterator_response['ShardIterator']

# Step 2: Continuously read records
print("Reading from stream...")
while True:
    response = kinesis.get_records(
        ShardIterator=shard_iterator,
        Limit=100  # Max records to fetch
    )
    
    records = response['Records']
    
    if records:
        print(f"\nüì¶ Received {len(records)} records:")
        for record in records:
            data = json.loads(record['Data'])
            print(f"  User: {data['user_id']}, Page: {data['page']}, Action: {data['action']}")
    
    # Get next iterator
    shard_iterator = response['NextShardIterator']
    
    # Sleep to avoid hitting API limits
    time.sleep(1)</code></pre>
            </div>

            <h2><span class="emoji">üé¨</span> Real-World Use Cases</h2>

            <h3>1. Clickstream Analytics</h3>

            <div class="example-box">
                <p><strong>Scenario:</strong> Track all user clicks on website, analyze in real-time</p>

                <div class="stream-diagram">
                    <div style="background: #10b981; color: white; padding: 10px; border-radius: 5px; margin: 10px;">
                        Website (Producer)
                    </div>
                    <div class="arrow">‚¨á</div>
                    <div class="shard-box">Kinesis Stream</div>
                    <div class="arrow">‚¨á</div>
                    <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 10px;">
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            Lambda: Real-time Dashboard
                        </div>
                        <div style="background: #8b5cf6; color: white; padding: 10px; border-radius: 5px;">
                            Kinesis Analytics: Aggregations
                        </div>
                    </div>
                </div>

                <h4>Producer Code (Website)</h4>
                <pre><code>import boto3
import json

kinesis = boto3.client('kinesis')

def track_click(user_id, page, element_clicked):
    """Track user click event"""
    kinesis.put_record(
        StreamName='website-clickstream',
        Data=json.dumps({
            'event_type': 'click',
            'user_id': user_id,
            'page': page,
            'element': element_clicked,
            'timestamp': int(time.time())
        }),
        PartitionKey=user_id
    )</code></pre>

                <h4>Consumer Code (Lambda for Analytics)</h4>
                <pre><code>import json
import boto3

cloudwatch = boto3.client('cloudwatch')

def lambda_handler(event, context):
    """Process clickstream data from Kinesis"""
    
    for record in event['Records']:
        # Decode record
        data = json.loads(record['kinesis']['data'])
        
        # Track metrics
        cloudwatch.put_metric_data(
            Namespace='Website/Clicks',
            MetricData=[{
                'MetricName': 'ClickCount',
                'Value': 1,
                'Dimensions': [
                    {'Name': 'Page', 'Value': data['page']},
                    {'Name': 'Element', 'Value': data['element']}
                ]
            }]
        )
        
        # Check for suspicious patterns (fraud detection)
        if is_bot_behavior(data):
            send_alert(data['user_id'])</code></pre>
            </div>

            <h3>2. IoT Telemetry Processing</h3>

            <div class="example-box">
                <p><strong>Scenario:</strong> Monitor temperature sensors across 10,000 devices</p>

                <pre><code>import boto3
import json

kinesis = boto3.client('kinesis')

def send_sensor_data(device_id, temperature, humidity):
    """IoT device sends sensor readings"""
    kinesis.put_record(
        StreamName='iot-sensor-data',
        Data=json.dumps({
            'device_id': device_id,
            'temperature': temperature,
            'humidity': humidity,
            'timestamp': int(time.time())
        }),
        PartitionKey=device_id  # Each device to consistent shard
    )

# Lambda consumer checks for anomalies
def lambda_handler(event, context):
    for record in event['Records']:
        data = json.loads(record['kinesis']['data'])
        
        # Alert on high temperature
        if data['temperature'] > 80:
            sns.publish(
                TopicArn='arn:aws:sns:us-east-1:123456789012:iot-alerts',
                Subject=f"‚ö†Ô∏è High temp on {data['device_id']}",
                Message=f"Temperature: {data['temperature']}¬∞F"
            )</code></pre>
            </div>

            <h3>3. Log Aggregation</h3>

            <div class="example-box">
                <p><strong>Scenario:</strong> Collect logs from 100 servers, analyze errors in real-time</p>

                <pre><code># Producer (on each server)
def log_to_kinesis(level, message, context):
    """Send log entries to Kinesis"""
    kinesis.put_record(
        StreamName='application-logs',
        Data=json.dumps({
            'level': level,
            'message': message,
            'server': os.getenv('HOSTNAME'),
            'context': context,
            'timestamp': datetime.utcnow().isoformat()
        }),
        PartitionKey=os.getenv('HOSTNAME')
    )

# Consumer (Lambda)
def lambda_handler(event, context):
    """Aggregate and alert on errors"""
    error_count = 0
    
    for record in event['Records']:
        data = json.loads(record['kinesis']['data'])
        
        if data['level'] == 'ERROR':
            error_count += 1
            # Store in DynamoDB for querying
            dynamodb.put_item(
                TableName='error-logs',
                Item={
                    'timestamp': data['timestamp'],
                    'server': data['server'],
                    'message': data['message']
                }
            )
    
    # Alert if too many errors
    if error_count > 10:
        send_pagerduty_alert(f"{error_count} errors detected")</code></pre>
            </div>

            <h2><span class="emoji">‚ö°</span> Advanced: Enhanced Fan-Out</h2>

            <div class="concept-box">
                <h3>The Read Throughput Problem</h3>
                <p><strong>Problem:</strong> Standard consumers share 2 MB/s per shard. If you have 3 consumers, each
                    gets ~0.66 MB/s.</p>

                <p><strong>Solution:</strong> <code>Enhanced Fan-Out</code> gives each consumer dedicated 2 MB/s
                    throughput.</p>

                <table>
                    <tr>
                        <th>Feature</th>
                        <th>Standard</th>
                        <th>Enhanced Fan-Out</th>
                    </tr>
                    <tr>
                        <td>Throughput per Consumer</td>
                        <td>Shared 2 MB/s</td>
                        <td>Dedicated 2 MB/s</td>
                    </tr>
                    <tr>
                        <td>Latency</td>
                        <td>~200ms (polling)</td>
                        <td>~70ms (push)</td>
                    </tr>
                    <tr>
                        <td>Cost</td>
                        <td>Lower</td>
                        <td>Higher ($0.015/shard-hour per consumer)</td>
                    </tr>
                </table>

                <h4>Enable Enhanced Fan-Out</h4>
                <pre><code>import boto3

kinesis = boto3.client('kinesis')

# Register consumer for enhanced fan-out
response = kinesis.register_stream_consumer(
    StreamARN='arn:aws:kinesis:us-east-1:123456789012:stream/website-clickstream',
    ConsumerName='analytics-consumer'
)

consumer_arn = response['Consumer']['ConsumerARN']
print(f"Consumer registered: {consumer_arn}")</code></pre>
            </div>

            <h2><span class="emoji">üé§</span> Interview Cheat Sheet</h2>

            <div class="analogy-box">
                <h3>Q: "What is Kinesis?"</h3>
                <p><strong>Your Answer:</strong></p>
                <p>"Kinesis is a real-time data streaming service. It's designed for high-throughput, continuous data
                    ingestion - like processing millions of clickstream events per second. Data is organized into
                    shards for parallel processing, and multiple consumers can read the same data stream. It's
                    different from SQS because data isn't deleted after consumption - it's retained for replay."</p>

                <h3>Q: "What's a shard?"</h3>
                <p><strong>Your Answer:</strong></p>
                <p>"A shard is a unit of capacity in Kinesis. Each shard provides 1 MB/s write and 2 MB/s read
                    throughput. When you create a stream, you specify the number of shards based on your expected
                    traffic. Partition keys determine which shard a record goes to - good partition keys ensure even
                    distribution across shards to avoid hot spots."</p>

                <h3>Q: "Kinesis vs SQS - when to use each?"</h3>
                <p><strong>Your Answer:</strong></p>
                <p>"Use Kinesis when you need high-throughput streaming with multiple consumers reading the same data -
                    like real-time analytics or IoT telemetry. Use SQS when you need a simple message queue where each
                    message is processed once - like job queues or decoupling microservices. Kinesis can handle
                    millions of events per second and retains data for replay; SQS is simpler and cheaper for standard
                    async processing."</p>

                <h3>Q: "How do you scale Kinesis?"</h3>
                <p><strong>Your Answer:</strong></p>
                <p>"You scale by adding shards. If you're hitting write limits (1 MB/s per shard), split shards to
                    double capacity. For read scaling, use enhanced fan-out to give each consumer dedicated throughput
                    instead of sharing. You can also use Kinesis Data Analytics to process data in the stream before
                    consumers read it. Monitor shard metrics in CloudWatch to detect hot shards."</p>
            </div>

            <h2><span class="emoji">üí°</span> Key Takeaways</h2>

            <ol>
                <li><strong>Mental Model:</strong> Kinesis = Highway with multiple lanes (shards) for continuous data
                    traffic</li>
                <li><strong>Core Value:</strong> Real-time, high-throughput streaming with data retention for replay
                </li>
                <li><strong>Shards:</strong> Unit of capacity (1 MB/s write, 2 MB/s read per shard)</li>
                <li><strong>Partition Key:</strong> Critical for even distribution - use high-cardinality keys</li>
                <li><strong>vs SQS:</strong> Kinesis for streaming analytics, SQS for job queues</li>
                <li><strong>Use Cases:</strong> Clickstream, IoT telemetry, log aggregation, real-time dashboards</li>
            </ol>

            <div class="success-box">
                <p style="font-size: 1.2em; text-align: center; margin: 0;">
                    <strong>Bottom Line:</strong> Kinesis turns massive streams of real-time data into actionable
                    insights. Use it when you need to process data <em>as it happens</em>, not hours later.
                </p>
            </div>
        </div>
    </div>
</body>

</html>