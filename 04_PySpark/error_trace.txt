Missing Python executable 'python3', defaulting to 'C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\pyspark\bin\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/01/08 06:22:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
26/01/08 06:22:48 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)
	at org.apache.spark.scheduler.Task.run(Task.scala:147)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified
	at java.base/java.lang.ProcessImpl.create(Native Method)
	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)
	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)
	... 34 more
26/01/08 06:22:48 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (work.lan executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)
	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)
	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)
	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)
	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)
	at org.apache.spark.scheduler.Task.run(Task.scala:147)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified
	at java.base/java.lang.ProcessImpl.create(Native Method)
	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)
	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)
	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)
	... 34 more

26/01/08 06:22:48 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
JAVA_HOME: C:\Users\ksank\.gemini\java\jdk-17.0.9+8
HADOOP_HOME: C:\Users\ksank\.hadoop
Session created.
DataFrame created. Attempting show()...

XXX CAUGHT ERROR XXX
An error occurred while calling o52.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (work.lan executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)

	at org.apache.spark.scheduler.Task.run(Task.scala:147)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessImpl.create(Native Method)

	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)

	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)

	... 34 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3003)

	at scala.Option.getOrElse(Option.scala:201)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3003)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2995)

	at scala.collection.immutable.List.foreach(List.scala:323)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2995)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)

	at scala.Option.foreach(Option.scala:437)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3278)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3209)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3198)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)

	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)

	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)

	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)

	at org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)

	at org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)

	at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)

	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)

	at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)

	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)

	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)

	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)

	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)

	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)

	at org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)

	at org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)

	at org.apache.spark.sql.Dataset.take(Dataset.scala:2814)

	at org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)

	at org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:568)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)

	at org.apache.spark.scheduler.Task.run(Task.scala:147)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	... 1 more

Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessImpl.create(Native Method)

	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)

	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)

	... 34 more

Traceback (most recent call last):
  File "C:\Users\ksank\training\04_PySpark\debug_show_error.py", line 31, in <module>
    df.show()
    ~~~~~~~^^
  File "C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\pyspark\sql\classic\dataframe.py", line 285, in show
    print(self._show_string(n, truncate, vertical))
          ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\pyspark\sql\classic\dataframe.py", line 303, in _show_string
    return self._jdf.showString(n, 20, vertical)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\py4j\java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
        answer, self.gateway_client, self.target_id, self.name)
  File "C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\pyspark\errors\exceptions\captured.py", line 263, in deco
    return f(*a, **kw)
  File "C:\Users\ksank\AppData\Roaming\Python\Python313\site-packages\py4j\protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
        "An error occurred while calling {0}{1}{2}.\n".
        format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o52.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (work.lan executor driver): java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)

	at org.apache.spark.scheduler.Task.run(Task.scala:147)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessImpl.create(Native Method)

	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)

	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)

	... 34 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3003)

	at scala.Option.getOrElse(Option.scala:201)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3003)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2995)

	at scala.collection.immutable.List.foreach(List.scala:323)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2995)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)

	at scala.Option.foreach(Option.scala:437)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3278)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3209)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3198)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1017)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2496)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2517)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2536)

	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)

	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)

	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)

	at org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2275)

	at org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1401)

	at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2265)

	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)

	at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2263)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:176)

	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:284)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:138)

	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)

	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)

	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)

	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:138)

	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:307)

	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:137)

	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:91)

	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)

	at org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2263)

	at org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1401)

	at org.apache.spark.sql.Dataset.take(Dataset.scala:2814)

	at org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:338)

	at org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:374)


	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:568)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: java.io.IOException: Cannot run program "python3": CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)

	at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)

	at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)

	at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)

	at org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)

	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)

	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)

	at org.apache.spark.rdd.RDD.iterator(RDD.scala:338)

	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)

	at org.apache.spark.scheduler.Task.run(Task.scala:147)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:679)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:682)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	... 1 more

Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified

	at java.base/java.lang.ProcessImpl.create(Native Method)

	at java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)

	at java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)

	at java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)

	... 34 more


SUCCESS: The process with PID 120028 (child process of PID 47268) has been terminated.
SUCCESS: The process with PID 47268 (child process of PID 61732) has been terminated.
SUCCESS: The process with PID 61732 (child process of PID 8428) has been terminated.
