Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/01/08 09:42:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
26/01/08 09:42:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.

[Stage 0:>                                                        (0 + 14) / 14]
26/01/08 09:42:43 ERROR Executor: Exception in task 8.0 in stage 0.0 (TID 8)
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)
	... 25 more
26/01/08 09:42:43 WARN TaskSetManager: Lost task 8.0 in stage 0.0 (TID 8) (work.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
	at scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.io.EOFException
	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)
	... 25 more

26/01/08 09:42:43 ERROR TaskSetManager: Task 8 in stage 0.0 failed 1 times; aborting job
Traceback (most recent call last):
Environment Configured. Temp Dir: C:\tmp\spark-temp
Python: C:\Users\ksank\training\04_PySpark\python312_env\python.exe
Spark Session Created. Version: 3.5.3
Testing collect()...
  File "C:\Users\ksank\training\04_PySpark\verify_python312_env.py", line 59, in <module>
    test_spark()
  File "C:\Users\ksank\training\04_PySpark\verify_python312_env.py", line 51, in test_spark
    rows = df.limit(5).rdd.collect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksank\training\04_PySpark\python312_env\Lib\site-packages\pyspark\rdd.py", line 1833, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\ksank\training\04_PySpark\python312_env\Lib\site-packages\py4j\java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "C:\Users\ksank\training\04_PySpark\python312_env\Lib\site-packages\pyspark\errors\exceptions\captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "C:\Users\ksank\training\04_PySpark\python312_env\Lib\site-packages\py4j\protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8) (work.lan executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 25 more


Driver stacktrace:

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)

	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)

	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)

	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)

	at scala.Option.foreach(Option.scala:407)

	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)

	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)

	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)

	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)

	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)

	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)

	at org.apache.spark.rdd.RDD.withScope(RDD.scala:410)

	at org.apache.spark.rdd.RDD.collect(RDD.scala:1048)

	at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)

	at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)

	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

	at java.base/java.lang.reflect.Method.invoke(Method.java:568)

	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)

	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)

	at py4j.Gateway.invoke(Gateway.java:282)

	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)

	at py4j.commands.CallCommand.execute(CallCommand.java:79)

	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)

	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)

	at java.base/java.lang.Thread.run(Thread.java:840)

Caused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)

	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)

	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)

	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)

	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)

	at scala.collection.Iterator$SliceIterator.hasNext(Iterator.scala:268)

	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)

	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)

	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)

	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)

	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)

	at org.apache.spark.scheduler.Task.run(Task.scala:141)

	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)

	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)

	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)

	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)

	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)

	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)

	... 1 more

Caused by: java.io.EOFException

	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:386)

	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)

	... 25 more



[Stage 0:>                                                        (0 + 13) / 14]
SUCCESS: The process with PID 91860 (child process of PID 34036) has been terminated.
SUCCESS: The process with PID 34036 (child process of PID 142124) has been terminated.
ERROR: The process with PID 142124 (child process of PID 46200) could not be terminated.
Reason: There is no running instance of the task.

SUCCESS: The process with PID 46200 (child process of PID 127968) has been terminated.
SUCCESS: The process with PID 127968 (child process of PID 135868) has been terminated.
