{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Complete Guide: Requests & BeautifulSoup\n",
                "\n",
                "## Comprehensive Reference for Web Scraping\n",
                "\n",
                "This notebook covers:\n",
                "- **Requests Library**: HTTP methods, response attributes, headers, sessions\n",
                "- **BeautifulSoup**: Parsing methods, navigation, searching, attributes\n",
                "- **Common Patterns**: Real-world examples and best practices\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1: Requests Library üåê\n",
                "\n",
                "## What is Requests?\n",
                "\n",
                "The `requests` library is used to send HTTP requests to web servers and receive responses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import json"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Basic HTTP Methods\n",
                "\n",
                "### GET Request (Most Common)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic GET request\n",
                "url = \"https://books.toscrape.com/\"\n",
                "response = requests.get(url)\n",
                "\n",
                "print(f\"Status Code: {response.status_code}\")\n",
                "print(f\"Response Type: {type(response)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### POST Request (Sending Data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# POST request with data\n",
                "post_url = \"https://httpbin.org/post\"\n",
                "data = {\n",
                "    'username': 'testuser',\n",
                "    'password': 'testpass'\n",
                "}\n",
                "\n",
                "response = requests.post(post_url, data=data)\n",
                "print(f\"Status: {response.status_code}\")\n",
                "print(f\"Response JSON: {response.json()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Other HTTP Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PUT - Update resource\n",
                "# response = requests.put(url, data=data)\n",
                "\n",
                "# DELETE - Delete resource\n",
                "# response = requests.delete(url)\n",
                "\n",
                "# PATCH - Partial update\n",
                "# response = requests.patch(url, data=data)\n",
                "\n",
                "# HEAD - Get headers only (no body)\n",
                "response = requests.head(url)\n",
                "print(f\"HEAD request headers: {response.headers}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Response Object Attributes\n",
                "\n",
                "### Essential Attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = requests.get(url)\n",
                "\n",
                "# Status code (200 = OK, 404 = Not Found, 500 = Server Error)\n",
                "print(f\"1. status_code: {response.status_code}\")\n",
                "\n",
                "# Boolean: True if status_code < 400\n",
                "print(f\"2. ok: {response.ok}\")\n",
                "\n",
                "# Response text (HTML as string)\n",
                "print(f\"3. text (first 100 chars): {response.text[:100]}\")\n",
                "\n",
                "# Response content (bytes)\n",
                "print(f\"4. content (first 100 bytes): {response.content[:100]}\")\n",
                "\n",
                "# Response URL (final URL after redirects)\n",
                "print(f\"5. url: {response.url}\")\n",
                "\n",
                "# Response headers (dictionary-like)\n",
                "print(f\"6. headers: {dict(response.headers)}\")\n",
                "\n",
                "# Encoding\n",
                "print(f\"7. encoding: {response.encoding}\")\n",
                "\n",
                "# Cookies\n",
                "print(f\"8. cookies: {response.cookies}\")\n",
                "\n",
                "# Elapsed time\n",
                "print(f\"9. elapsed: {response.elapsed}\")\n",
                "\n",
                "# Request history (redirects)\n",
                "print(f\"10. history: {response.history}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Response Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test JSON endpoint\n",
                "json_url = \"https://httpbin.org/json\"\n",
                "response = requests.get(json_url)\n",
                "\n",
                "# Parse JSON response\n",
                "print(\"1. json() - Parse JSON:\")\n",
                "print(response.json())\n",
                "\n",
                "# Raise exception for bad status codes\n",
                "print(\"\\n2. raise_for_status() - Check for errors:\")\n",
                "try:\n",
                "    response.raise_for_status()\n",
                "    print(\"   No errors!\")\n",
                "except requests.exceptions.HTTPError as e:\n",
                "    print(f\"   Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Request Parameters\n",
                "\n",
                "### Query Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# URL with query parameters\n",
                "params_url = \"https://httpbin.org/get\"\n",
                "params = {\n",
                "    'search': 'python',\n",
                "    'page': 1,\n",
                "    'limit': 10\n",
                "}\n",
                "\n",
                "response = requests.get(params_url, params=params)\n",
                "print(f\"Final URL: {response.url}\")\n",
                "print(f\"Response: {response.json()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Headers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Custom headers\n",
                "headers = {\n",
                "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
                "    'Accept': 'text/html,application/xhtml+xml',\n",
                "    'Accept-Language': 'en-US,en;q=0.9',\n",
                "    'Referer': 'https://www.google.com/'\n",
                "}\n",
                "\n",
                "response = requests.get(url, headers=headers)\n",
                "print(f\"Request sent with custom headers\")\n",
                "print(f\"Status: {response.status_code}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cookies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Send cookies with request\n",
                "cookies = {\n",
                "    'session_id': 'abc123',\n",
                "    'user_token': 'xyz789'\n",
                "}\n",
                "\n",
                "cookie_url = \"https://httpbin.org/cookies\"\n",
                "response = requests.get(cookie_url, cookies=cookies)\n",
                "print(f\"Cookies sent: {response.json()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Timeout"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set timeout (in seconds)\n",
                "try:\n",
                "    response = requests.get(url, timeout=5)\n",
                "    print(f\"Request completed in {response.elapsed.total_seconds():.2f} seconds\")\n",
                "except requests.exceptions.Timeout:\n",
                "    print(\"Request timed out!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Sessions (Persistent Connections)\n",
                "\n",
                "Sessions maintain cookies and settings across requests."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a session\n",
                "session = requests.Session()\n",
                "\n",
                "# Set default headers for all requests\n",
                "session.headers.update({\n",
                "    'User-Agent': 'My Scraper 1.0'\n",
                "})\n",
                "\n",
                "# Make requests with session\n",
                "response1 = session.get(url)\n",
                "response2 = session.get(url)\n",
                "\n",
                "print(f\"Session cookies: {session.cookies}\")\n",
                "\n",
                "# Close session\n",
                "session.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5 Error Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    response = requests.get(url, timeout=5)\n",
                "    response.raise_for_status()  # Raise exception for 4xx/5xx status codes\n",
                "    \nexcept requests.exceptions.HTTPError as e:\n",
                "    print(f\"HTTP Error: {e}\")\n",
                "    \n",
                "except requests.exceptions.ConnectionError as e:\n",
                "    print(f\"Connection Error: {e}\")\n",
                "    \n",
                "except requests.exceptions.Timeout as e:\n",
                "    print(f\"Timeout Error: {e}\")\n",
                "    \n",
                "except requests.exceptions.RequestException as e:\n",
                "    print(f\"General Error: {e}\")\n",
                "    \n",
                "else:\n",
                "    print(f\"Success! Status: {response.status_code}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 2: BeautifulSoup üçú\n",
                "\n",
                "## What is BeautifulSoup?\n",
                "\n",
                "BeautifulSoup parses HTML/XML and creates a navigable tree structure."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Creating a Soup Object"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fetch HTML\n",
                "response = requests.get(url)\n",
                "html = response.text\n",
                "\n",
                "# Create soup object\n",
                "# Parsers: 'html.parser' (built-in), 'lxml' (fast), 'html5lib' (lenient)\n",
                "soup = BeautifulSoup(html, 'lxml')\n",
                "\n",
                "print(f\"Soup type: {type(soup)}\")\n",
                "print(f\"Title: {soup.title}\")\n",
                "print(f\"Title text: {soup.title.string}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Finding Elements\n",
                "\n",
                "### find() - Find First Match"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find first <h1> tag\n",
                "h1 = soup.find('h1')\n",
                "print(f\"First h1: {h1}\")\n",
                "print(f\"h1 text: {h1.get_text() if h1 else 'Not found'}\")\n",
                "\n",
                "# Find by class\n",
                "price = soup.find('p', class_='price_color')\n",
                "print(f\"\\nFirst price: {price}\")\n",
                "\n",
                "# Find by id\n",
                "element = soup.find(id='some-id')\n",
                "print(f\"\\nElement by ID: {element}\")\n",
                "\n",
                "# Find by attributes\n",
                "link = soup.find('a', href=True)  # Find <a> with href attribute\n",
                "print(f\"\\nFirst link: {link}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### find_all() - Find All Matches"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all <h3> tags\n",
                "all_h3 = soup.find_all('h3')\n",
                "print(f\"Found {len(all_h3)} h3 tags\")\n",
                "\n",
                "# Find all with class\n",
                "all_prices = soup.find_all('p', class_='price_color')\n",
                "print(f\"Found {len(all_prices)} prices\")\n",
                "\n",
                "# Find multiple tags\n",
                "headers = soup.find_all(['h1', 'h2', 'h3'])\n",
                "print(f\"Found {len(headers)} headers (h1, h2, h3)\")\n",
                "\n",
                "# Limit results\n",
                "first_5_links = soup.find_all('a', limit=5)\n",
                "print(f\"First 5 links: {len(first_5_links)}\")\n",
                "\n",
                "# Find with attributes\n",
                "images = soup.find_all('img', src=True)\n",
                "print(f\"Found {len(images)} images with src attribute\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### select() - CSS Selectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CSS selector examples\n",
                "\n",
                "# By tag\n",
                "divs = soup.select('div')\n",
                "print(f\"1. All divs: {len(divs)}\")\n",
                "\n",
                "# By class (use .classname)\n",
                "prices = soup.select('.price_color')\n",
                "print(f\"2. By class .price_color: {len(prices)}\")\n",
                "\n",
                "# By ID (use #id)\n",
                "element = soup.select('#some-id')\n",
                "print(f\"3. By ID #some-id: {len(element)}\")\n",
                "\n",
                "# Descendant selector\n",
                "article_h3 = soup.select('article h3')\n",
                "print(f\"4. h3 inside article: {len(article_h3)}\")\n",
                "\n",
                "# Direct child (>)\n",
                "direct_children = soup.select('div > p')\n",
                "print(f\"5. p directly inside div: {len(direct_children)}\")\n",
                "\n",
                "# Attribute selector\n",
                "links_with_href = soup.select('a[href]')\n",
                "print(f\"6. Links with href: {len(links_with_href)}\")\n",
                "\n",
                "# Attribute value\n",
                "specific_links = soup.select('a[href=\"/index.html\"]')\n",
                "print(f\"7. Links to /index.html: {len(specific_links)}\")\n",
                "\n",
                "# Multiple classes\n",
                "elements = soup.select('.class1.class2')\n",
                "print(f\"8. Elements with both classes: {len(elements)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### select_one() - First CSS Match"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find first match with CSS selector\n",
                "first_price = soup.select_one('.price_color')\n",
                "print(f\"First price: {first_price}\")\n",
                "print(f\"Price text: {first_price.get_text() if first_price else 'Not found'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Navigating the Tree\n",
                "\n",
                "### Parent, Children, Siblings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get first article\n",
                "article = soup.find('article')\n",
                "\n",
                "if article:\n",
                "    # Parent\n",
                "    print(f\"1. Parent tag: {article.parent.name}\")\n",
                "    \n",
                "    # Children (direct descendants)\n",
                "    print(f\"\\n2. Children:\")\n",
                "    for i, child in enumerate(article.children, 1):\n",
                "        if child.name:  # Skip text nodes\n",
                "            print(f\"   {i}. {child.name}\")\n",
                "    \n",
                "    # Descendants (all nested elements)\n",
                "    print(f\"\\n3. Total descendants: {len(list(article.descendants))}\")\n",
                "    \n",
                "    # Next sibling\n",
                "    next_sib = article.find_next_sibling()\n",
                "    print(f\"\\n4. Next sibling: {next_sib.name if next_sib else 'None'}\")\n",
                "    \n",
                "    # Previous sibling\n",
                "    prev_sib = article.find_previous_sibling()\n",
                "    print(f\"5. Previous sibling: {prev_sib.name if prev_sib else 'None'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Finding Next/Previous Elements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find first h3\n",
                "h3 = soup.find('h3')\n",
                "\n",
                "if h3:\n",
                "    # Find next <p> tag\n",
                "    next_p = h3.find_next('p')\n",
                "    print(f\"Next <p> after h3: {next_p}\")\n",
                "    \n",
                "    # Find previous <div>\n",
                "    prev_div = h3.find_previous('div')\n",
                "    print(f\"\\nPrevious <div> before h3: {prev_div.name if prev_div else 'None'}\")\n",
                "    \n",
                "    # Find all next siblings\n",
                "    next_siblings = h3.find_next_siblings()\n",
                "    print(f\"\\nNext siblings: {len(next_siblings)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Extracting Data\n",
                "\n",
                "### Getting Text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "element = soup.find('h1')\n",
                "\n",
                "if element:\n",
                "    # Get text (with whitespace)\n",
                "    text1 = element.get_text()\n",
                "    print(f\"1. get_text(): '{text1}'\")\n",
                "    \n",
                "    # Get text (stripped)\n",
                "    text2 = element.get_text(strip=True)\n",
                "    print(f\"2. get_text(strip=True): '{text2}'\")\n",
                "    \n",
                "    # Get text with separator\n",
                "    text3 = element.get_text(separator=' | ')\n",
                "    print(f\"3. get_text(separator=' | '): '{text3}'\")\n",
                "    \n",
                "    # Using .string (only if element has single string)\n",
                "    text4 = element.string\n",
                "    print(f\"4. .string: '{text4}'\")\n",
                "    \n",
                "    # Using .text (shortcut for get_text())\n",
                "    text5 = element.text\n",
                "    print(f\"5. .text: '{text5}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Getting Attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find a link\n",
                "link = soup.find('a')\n",
                "\n",
                "if link:\n",
                "    # Get attribute using dictionary syntax\n",
                "    href1 = link['href']\n",
                "    print(f\"1. link['href']: {href1}\")\n",
                "    \n",
                "    # Get attribute using .get() (safer, returns None if not found)\n",
                "    href2 = link.get('href')\n",
                "    print(f\"2. link.get('href'): {href2}\")\n",
                "    \n",
                "    # Get with default value\n",
                "    title = link.get('title', 'No title')\n",
                "    print(f\"3. link.get('title', 'No title'): {title}\")\n",
                "    \n",
                "    # Get all attributes\n",
                "    attrs = link.attrs\n",
                "    print(f\"4. All attributes: {attrs}\")\n",
                "    \n",
                "    # Check if attribute exists\n",
                "    has_href = link.has_attr('href')\n",
                "    print(f\"5. Has href attribute: {has_href}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Getting Class Names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find element with classes\n",
                "element = soup.find('p', class_='price_color')\n",
                "\n",
                "if element:\n",
                "    # Get classes as list\n",
                "    classes = element.get('class', [])\n",
                "    print(f\"Classes: {classes}\")\n",
                "    \n",
                "    # Check if has specific class\n",
                "    has_class = 'price_color' in element.get('class', [])\n",
                "    print(f\"Has 'price_color' class: {has_class}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Advanced Searching\n",
                "\n",
                "### Using Functions as Filters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all tags with more than 2 attributes\n",
                "def has_many_attrs(tag):\n",
                "    return len(tag.attrs) > 2\n",
                "\n",
                "elements = soup.find_all(has_many_attrs)\n",
                "print(f\"Elements with >2 attributes: {len(elements)}\")\n",
                "\n",
                "# Find all links to external sites\n",
                "def is_external_link(tag):\n",
                "    return tag.name == 'a' and tag.get('href', '').startswith('http')\n",
                "\n",
                "external_links = soup.find_all(is_external_link)\n",
                "print(f\"External links: {len(external_links)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using Regular Expressions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "# Find tags matching regex\n",
                "headers = soup.find_all(re.compile('^h[1-6]$'))  # h1, h2, h3, h4, h5, h6\n",
                "print(f\"All headers: {len(headers)}\")\n",
                "\n",
                "# Find by attribute value matching regex\n",
                "price_elements = soup.find_all('p', class_=re.compile('price'))\n",
                "print(f\"Elements with 'price' in class: {len(price_elements)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.6 Practical Examples\n",
                "\n",
                "### Example 1: Extract All Links"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all links\n",
                "links = soup.find_all('a', href=True)\n",
                "\n",
                "print(f\"Found {len(links)} links\\n\")\n",
                "\n",
                "for i, link in enumerate(links[:5], 1):\n",
                "    href = link['href']\n",
                "    text = link.get_text(strip=True)\n",
                "    print(f\"{i}. {text[:30]:30} ‚Üí {href}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 2: Extract Table Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example HTML with table\n",
                "table_html = \"\"\"\n",
                "<table>\n",
                "    <tr>\n",
                "        <th>Name</th>\n",
                "        <th>Age</th>\n",
                "        <th>City</th>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>Alice</td>\n",
                "        <td>25</td>\n",
                "        <td>NYC</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <td>Bob</td>\n",
                "        <td>30</td>\n",
                "        <td>LA</td>\n",
                "    </tr>\n",
                "</table>\n",
                "\"\"\"\n",
                "\n",
                "table_soup = BeautifulSoup(table_html, 'lxml')\n",
                "table = table_soup.find('table')\n",
                "\n",
                "# Extract headers\n",
                "headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
                "print(f\"Headers: {headers}\")\n",
                "\n",
                "# Extract rows\n",
                "rows = []\n",
                "for tr in table.find_all('tr')[1:]:  # Skip header row\n",
                "    cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
                "    rows.append(cells)\n",
                "\n",
                "print(f\"\\nRows:\")\n",
                "for row in rows:\n",
                "    print(row)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example 3: Extract Nested Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all book articles\n",
                "books = soup.find_all('article', class_='product_pod')\n",
                "\n",
                "print(f\"Found {len(books)} books\\n\")\n",
                "\n",
                "for i, book in enumerate(books[:3], 1):\n",
                "    # Title (nested in h3 > a)\n",
                "    title = book.find('h3').find('a')['title']\n",
                "    \n",
                "    # Price (in p.price_color)\n",
                "    price = book.find('p', class_='price_color').get_text()\n",
                "    \n",
                "    # Rating (class name)\n",
                "    rating_class = book.find('p', class_='star-rating')['class'][1]\n",
                "    \n",
                "    print(f\"{i}. {title}\")\n",
                "    print(f\"   Price: {price}\")\n",
                "    print(f\"   Rating: {rating_class}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.7 Common Attributes Summary\n",
                "\n",
                "### Tag Attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "element = soup.find('article')\n",
                "\n",
                "if element:\n",
                "    print(\"Tag Attributes:\")\n",
                "    print(f\"1. .name: {element.name}\")  # Tag name\n",
                "    print(f\"2. .attrs: {element.attrs}\")  # All attributes\n",
                "    print(f\"3. .string: {element.string}\")  # Direct string content\n",
                "    print(f\"4. .text: {element.text[:50]}\")  # All text content\n",
                "    print(f\"5. .parent: {element.parent.name if element.parent else None}\")  # Parent tag\n",
                "    print(f\"6. .contents: {len(element.contents)}\")  # Direct children\n",
                "    print(f\"7. .children: {len(list(element.children))}\")  # Direct children iterator\n",
                "    print(f\"8. .descendants: {len(list(element.descendants))}\")  # All descendants"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì Quick Reference\n",
                "\n",
                "### Requests Cheat Sheet\n",
                "\n",
                "```python\n",
                "# Basic request\n",
                "response = requests.get(url)\n",
                "\n",
                "# With parameters\n",
                "response = requests.get(url, params={'key': 'value'})\n",
                "\n",
                "# With headers\n",
                "response = requests.get(url, headers={'User-Agent': 'MyBot'})\n",
                "\n",
                "# With timeout\n",
                "response = requests.get(url, timeout=5)\n",
                "\n",
                "# Response attributes\n",
                "response.status_code  # HTTP status\n",
                "response.text         # HTML as string\n",
                "response.content      # HTML as bytes\n",
                "response.json()       # Parse JSON\n",
                "response.headers      # Response headers\n",
                "response.cookies      # Cookies\n",
                "```\n",
                "\n",
                "### BeautifulSoup Cheat Sheet\n",
                "\n",
                "```python\n",
                "# Create soup\n",
                "soup = BeautifulSoup(html, 'lxml')\n",
                "\n",
                "# Finding\n",
                "soup.find('tag')                    # First match\n",
                "soup.find_all('tag')                # All matches\n",
                "soup.find('tag', class_='name')     # By class\n",
                "soup.find(id='name')                # By ID\n",
                "soup.select('.class')               # CSS selector\n",
                "soup.select_one('#id')              # First CSS match\n",
                "\n",
                "# Extracting\n",
                "tag.get_text()                      # Get text\n",
                "tag['attribute']                    # Get attribute\n",
                "tag.get('attribute', 'default')     # Safe get\n",
                "tag.attrs                           # All attributes\n",
                "\n",
                "# Navigation\n",
                "tag.parent                          # Parent element\n",
                "tag.children                        # Direct children\n",
                "tag.find_next_sibling()             # Next sibling\n",
                "tag.find_previous_sibling()         # Previous sibling\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}