{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üï∑Ô∏è Web Scraping Training with Python\n",
                "\n",
                "## Scraping Books to Scrape Website\n",
                "\n",
                "**Target Website:** https://books.toscrape.com/\n",
                "\n",
                "**Learning Objectives:**\n",
                "- Understand HTML structure and DOM navigation\n",
                "- Use BeautifulSoup for parsing HTML\n",
                "- Extract various elements (titles, prices, ratings, images)\n",
                "- Handle pagination and multiple pages\n",
                "- Save data to CSV and JSON formats\n",
                "- Best practices and ethical scraping\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 1: Install Required Libraries\n",
                "\n",
                "We'll need:\n",
                "- **requests**: To fetch web pages\n",
                "- **beautifulsoup4**: To parse HTML\n",
                "- **pandas**: To organize and export data\n",
                "- **lxml**: Parser for BeautifulSoup (faster than default)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries (run once)\n",
                "!pip install requests beautifulsoup4 pandas lxml"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö Step 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import time\n",
                "from urllib.parse import urljoin\n",
                "import json\n",
                "import re  # For regex to clean prices"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üåê Step 3: Fetch a Web Page\n",
                "\n",
                "**How it works:**\n",
                "1. `requests.get()` sends an HTTP GET request\n",
                "2. Server responds with HTML content\n",
                "3. We check the status code (200 = success)\n",
                "4. Access HTML via `.text` attribute"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Status Code: 200\n",
                        "Content Length: 51294 characters\n",
                        "\n",
                        "HTML Preview:\n",
                        "<!DOCTYPE html>\n",
                        "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
                        "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
                        "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
                        "<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\n",
                        "    <head>\n",
                        "        <title>\n",
                        "    All products | Books to Scrape - Sandbox\n",
                        "</title>\n",
                        "\n",
                        "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" /\n"
                    ]
                }
            ],
            "source": [
                "# Define the URL\n",
                "url = \"https://books.toscrape.com/\"\n",
                "\n",
                "# Send GET request\n",
                "response = requests.get(url)\n",
                "\n",
                "# Check if request was successful\n",
                "print(f\"Status Code: {response.status_code}\")\n",
                "print(f\"Content Length: {len(response.text)} characters\")\n",
                "\n",
                "# Preview first 500 characters\n",
                "print(\"\\nHTML Preview:\")\n",
                "print(response.text[:500])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç Step 4: Parse HTML with BeautifulSoup\n",
                "\n",
                "**BeautifulSoup** converts raw HTML into a navigable tree structure.\n",
                "\n",
                "**Key Methods:**\n",
                "- `.find()` - Find first matching element\n",
                "- `.find_all()` - Find all matching elements\n",
                "- `.select()` - Use CSS selectors\n",
                "- `.get_text()` - Extract text content\n",
                "- `.get('attribute')` - Get attribute value"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<!DOCTYPE html>\n",
                        "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
                        "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
                        "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
                        "<!--[if gt IE 8]><!-->\n",
                        "<html class=\"no-js\" lang=\"en-us\">\n",
                        " <!--<![endif]-->\n",
                        " <head>\n",
                        "  <title>\n",
                        "   All products | Books to Scrape - Sandbox\n",
                        "  </title>\n",
                        "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
                        "  <meta content=\"24th Jun 2016 09:29\" name=\"created\"/>\n",
                        "  <meta content=\"\" name=\"description\"/>\n",
                        "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
                        "  <meta content=\"NOARCHIVE,NOCACHE\" name=\"robots\"/>\n",
                        "  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\n",
                        "  <!--[if lt IE 9]>\n",
                        "        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\n",
                        "        <![endif]-->\n",
                        "  <link href=\"static/oscar/favicon.ico\" rel=\"shortcut icon\"/>\n",
                        "  <link href=\"static/oscar/css/styles.css\" rel=\"stylesheet\" type=\"tex\n"
                    ]
                }
            ],
            "source": [
                "# Parse HTML\n",
                "soup = BeautifulSoup(response.text, 'lxml')\n",
                "\n",
                "# Pretty print the HTML (first 1000 characters)\n",
                "print(soup.prettify()[:1000])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìñ Step 5: Extract Book Titles\n",
                "\n",
                "**Inspection Process:**\n",
                "1. Right-click on a book title ‚Üí Inspect Element\n",
                "2. Find the HTML structure: `<h3><a title=\"Book Name\">...</a></h3>`\n",
                "3. Use BeautifulSoup to find all `<h3>` tags\n",
                "4. Extract the `title` attribute from `<a>` tags"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20 books on this page\n",
                        "\n",
                        "1. A Light in the Attic\n",
                        "2. Tipping the Velvet\n",
                        "3. Soumission\n",
                        "4. Sharp Objects\n",
                        "5. Sapiens: A Brief History of Humankind\n"
                    ]
                }
            ],
            "source": [
                "# Find all book titles\n",
                "book_titles = soup.find_all('h3')\n",
                "\n",
                "print(f\"Found {len(book_titles)} books on this page\\n\")\n",
                "\n",
                "# Extract and display titles\n",
                "titles = []\n",
                "for i, book in enumerate(book_titles[:5], 1):  # Show first 5\n",
                "    title = book.find('a')['title']\n",
                "    titles.append(title)\n",
                "    print(f\"{i}. {title}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üí∞ Step 6: Extract Book Prices\n",
                "\n",
                "**HTML Structure:**\n",
                "```html\n",
                "<p class=\"price_color\">¬£51.77</p>\n",
                "```\n",
                "\n",
                "**Extraction Steps:**\n",
                "1. Find all elements with class `price_color`\n",
                "2. Get text content\n",
                "3. Clean the price using regex to extract only numbers\n",
                "4. Convert to float for calculations\n",
                "\n",
                "**Note:** We use regex to handle encoding issues with the ¬£ symbol"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20 prices\n",
                        "\n",
                        "1. √Ç¬£51.77 ‚Üí ¬£51.77\n",
                        "2. √Ç¬£53.74 ‚Üí ¬£53.74\n",
                        "3. √Ç¬£50.10 ‚Üí ¬£50.1\n",
                        "4. √Ç¬£47.82 ‚Üí ¬£47.82\n",
                        "5. √Ç¬£54.23 ‚Üí ¬£54.23\n"
                    ]
                }
            ],
            "source": [
                "def clean_price(price_text):\n",
                "    \"\"\"\n",
                "    Extract numeric price from text, handling encoding issues.\n",
                "    \n",
                "    Args:\n",
                "        price_text (str): Raw price text (e.g., '¬£51.77' or '√Ç¬£51.77')\n",
                "    \n",
                "    Returns:\n",
                "        float: Cleaned price as a number\n",
                "    \"\"\"\n",
                "    # Use regex to extract only numbers and decimal point\n",
                "    match = re.search(r'\\d+\\.\\d+', price_text)\n",
                "    if match:\n",
                "        return float(match.group())\n",
                "    return 0.0\n",
                "\n",
                "# Find all prices\n",
                "prices = soup.find_all('p', class_='price_color')\n",
                "\n",
                "print(f\"Found {len(prices)} prices\\n\")\n",
                "\n",
                "# Extract and clean prices\n",
                "price_list = []\n",
                "for i, price in enumerate(prices[:5], 1):  # Show first 5\n",
                "    price_text = price.get_text()\n",
                "    price_value = clean_price(price_text)\n",
                "    price_list.append(price_value)\n",
                "    print(f\"{i}. {price_text} ‚Üí ¬£{price_value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚≠ê Step 7: Extract Star Ratings\n",
                "\n",
                "**HTML Structure:**\n",
                "```html\n",
                "<p class=\"star-rating Three\">\n",
                "```\n",
                "\n",
                "**Rating Mapping:**\n",
                "- One ‚Üí 1 star\n",
                "- Two ‚Üí 2 stars\n",
                "- Three ‚Üí 3 stars\n",
                "- Four ‚Üí 4 stars\n",
                "- Five ‚Üí 5 stars"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20 ratings\n",
                        "\n",
                        "1. Three ‚Üí 3 stars\n",
                        "2. One ‚Üí 1 stars\n",
                        "3. One ‚Üí 1 stars\n",
                        "4. Four ‚Üí 4 stars\n",
                        "5. Five ‚Üí 5 stars\n"
                    ]
                }
            ],
            "source": [
                "# Find all star ratings\n",
                "ratings = soup.find_all('p', class_='star-rating')\n",
                "\n",
                "# Rating conversion dictionary\n",
                "rating_map = {\n",
                "    'One': 1,\n",
                "    'Two': 2,\n",
                "    'Three': 3,\n",
                "    'Four': 4,\n",
                "    'Five': 5\n",
                "}\n",
                "\n",
                "print(f\"Found {len(ratings)} ratings\\n\")\n",
                "\n",
                "# Extract ratings\n",
                "rating_list = []\n",
                "for i, rating in enumerate(ratings[:5], 1):  # Show first 5\n",
                "    # Get the second class name (e.g., 'Three' from 'star-rating Three')\n",
                "    rating_class = rating['class'][1]\n",
                "    rating_value = rating_map[rating_class]\n",
                "    rating_list.append(rating_value)\n",
                "    print(f\"{i}. {rating_class} ‚Üí {rating_value} stars\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 8: Extract Availability\n",
                "\n",
                "**HTML Structure:**\n",
                "```html\n",
                "<p class=\"instock availability\">\n",
                "    <i class=\"icon-ok\"></i>\n",
                "    In stock\n",
                "</p>\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20 availability statuses\n",
                        "\n",
                        "1. In stock\n",
                        "2. In stock\n",
                        "3. In stock\n",
                        "4. In stock\n",
                        "5. In stock\n"
                    ]
                }
            ],
            "source": [
                "# Find all availability info\n",
                "availability = soup.find_all('p', class_='instock availability')\n",
                "\n",
                "print(f\"Found {len(availability)} availability statuses\\n\")\n",
                "\n",
                "# Extract availability\n",
                "availability_list = []\n",
                "for i, avail in enumerate(availability[:5], 1):  # Show first 5\n",
                "    status = avail.get_text(strip=True)\n",
                "    availability_list.append(status)\n",
                "    print(f\"{i}. {status}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üñºÔ∏è Step 9: Extract Book Image URLs\n",
                "\n",
                "**HTML Structure:**\n",
                "```html\n",
                "<div class=\"image_container\">\n",
                "    <a href=\"...\">\n",
                "        <img src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\" ...>\n",
                "    </a>\n",
                "</div>\n",
                "```\n",
                "\n",
                "**Note:** Image URLs are relative, so we need to join them with the base URL."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find all book images\n",
                "images = soup.find_all('div', class_='image_container')\n",
                "\n",
                "print(f\"Found {len(images)} images\\n\")\n",
                "\n",
                "# Extract image URLs\n",
                "image_urls = []\n",
                "for i, img_container in enumerate(images[:5], 1):  # Show first 5\n",
                "    img_tag = img_container.find('img')\n",
                "    relative_url = img_tag['src']\n",
                "    # Convert relative URL to absolute URL\n",
                "    absolute_url = urljoin(url, relative_url)\n",
                "    image_urls.append(absolute_url)\n",
                "    print(f\"{i}. {absolute_url}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîó Step 10: Extract Book Detail Page Links\n",
                "\n",
                "Each book has a link to its detail page with more information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 20 book links\n",
                        "\n",
                        "1. https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
                        "2. https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\n",
                        "3. https://books.toscrape.com/catalogue/soumission_998/index.html\n",
                        "4. https://books.toscrape.com/catalogue/sharp-objects_997/index.html\n",
                        "5. https://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html\n"
                    ]
                }
            ],
            "source": [
                "# Find all book detail links\n",
                "book_links = soup.find_all('h3')\n",
                "\n",
                "print(f\"Found {len(book_links)} book links\\n\")\n",
                "\n",
                "# Extract URLs\n",
                "detail_urls = []\n",
                "for i, book in enumerate(book_links[:5], 1):  # Show first 5\n",
                "    link = book.find('a')['href']\n",
                "    absolute_link = urljoin(url, link)\n",
                "    detail_urls.append(absolute_link)\n",
                "    print(f\"{i}. {absolute_link}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Step 11: Complete Function to Scrape All Books on a Page\n",
                "\n",
                "Let's combine everything into a reusable function with proper price cleaning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scraped 20 books\n",
                        "\n",
                        "First book:\n",
                        "{\n",
                        "  \"title\": \"A Light in the Attic\",\n",
                        "  \"price\": 51.77,\n",
                        "  \"rating\": 3,\n",
                        "  \"availability\": \"In stock\",\n",
                        "  \"image_url\": \"https://books.toscrape.com/media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\",\n",
                        "  \"detail_url\": \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "def scrape_books_page(page_url):\n",
                "    \"\"\"\n",
                "    Scrape all books from a single page.\n",
                "    \n",
                "    Args:\n",
                "        page_url (str): URL of the page to scrape\n",
                "    \n",
                "    Returns:\n",
                "        list: List of dictionaries containing book information\n",
                "    \"\"\"\n",
                "    # Fetch the page\n",
                "    response = requests.get(page_url)\n",
                "    soup = BeautifulSoup(response.text, 'lxml')\n",
                "    \n",
                "    # Find all book containers\n",
                "    books = soup.find_all('article', class_='product_pod')\n",
                "    \n",
                "    books_data = []\n",
                "    \n",
                "    for book in books:\n",
                "        # Extract title\n",
                "        title = book.find('h3').find('a')['title']\n",
                "        \n",
                "        # Extract price with proper cleaning\n",
                "        price_text = book.find('p', class_='price_color').get_text()\n",
                "        price = clean_price(price_text)\n",
                "        \n",
                "        # Extract rating\n",
                "        rating_class = book.find('p', class_='star-rating')['class'][1]\n",
                "        rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
                "        rating = rating_map[rating_class]\n",
                "        \n",
                "        # Extract availability\n",
                "        availability = book.find('p', class_='instock availability').get_text(strip=True)\n",
                "        \n",
                "        # Extract image URL\n",
                "        img_url = urljoin(page_url, book.find('img')['src'])\n",
                "        \n",
                "        # Extract detail page URL\n",
                "        detail_url = urljoin(page_url, book.find('h3').find('a')['href'])\n",
                "        \n",
                "        # Create book dictionary\n",
                "        book_data = {\n",
                "            'title': title,\n",
                "            'price': price,\n",
                "            'rating': rating,\n",
                "            'availability': availability,\n",
                "            'image_url': img_url,\n",
                "            'detail_url': detail_url\n",
                "        }\n",
                "        \n",
                "        books_data.append(book_data)\n",
                "    \n",
                "    return books_data\n",
                "\n",
                "# Test the function\n",
                "books = scrape_books_page(url)\n",
                "print(f\"Scraped {len(books)} books\\n\")\n",
                "print(\"First book:\")\n",
                "print(json.dumps(books[0], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÑ Step 12: Handle Pagination (Multiple Pages)\n",
                "\n",
                "The website has multiple pages. Let's scrape all of them!\n",
                "\n",
                "**Pagination Pattern:**\n",
                "- Page 1: `https://books.toscrape.com/catalogue/page-1.html`\n",
                "- Page 2: `https://books.toscrape.com/catalogue/page-2.html`\n",
                "- etc."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scraping page 1: https://books.toscrape.com/\n",
                        "  ‚Üí Found 20 books\n",
                        "Scraping page 2: https://books.toscrape.com/catalogue/page-2.html\n",
                        "  ‚Üí Found 20 books\n",
                        "Scraping page 3: https://books.toscrape.com/catalogue/page-3.html\n",
                        "  ‚Üí Found 20 books\n",
                        "\n",
                        "Total books scraped: 60\n"
                    ]
                }
            ],
            "source": [
                "def scrape_all_pages(base_url, max_pages=5):\n",
                "    \"\"\"\n",
                "    Scrape books from multiple pages.\n",
                "    \n",
                "    Args:\n",
                "        base_url (str): Base URL of the website\n",
                "        max_pages (int): Maximum number of pages to scrape\n",
                "    \n",
                "    Returns:\n",
                "        list: Combined list of all books from all pages\n",
                "    \"\"\"\n",
                "    all_books = []\n",
                "    \n",
                "    for page_num in range(1, max_pages + 1):\n",
                "        # Construct page URL\n",
                "        if page_num == 1:\n",
                "            page_url = base_url\n",
                "        else:\n",
                "            page_url = f\"{base_url}catalogue/page-{page_num}.html\"\n",
                "        \n",
                "        print(f\"Scraping page {page_num}: {page_url}\")\n",
                "        \n",
                "        try:\n",
                "            # Scrape the page\n",
                "            books = scrape_books_page(page_url)\n",
                "            all_books.extend(books)\n",
                "            print(f\"  ‚Üí Found {len(books)} books\")\n",
                "            \n",
                "            # Be polite: wait 1 second between requests\n",
                "            time.sleep(1)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  ‚Üí Error: {e}\")\n",
                "            break\n",
                "    \n",
                "    return all_books\n",
                "\n",
                "# Scrape first 3 pages\n",
                "all_books = scrape_all_pages(url, max_pages=3)\n",
                "print(f\"\\nTotal books scraped: {len(all_books)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Step 13: Convert to Pandas DataFrame\n",
                "\n",
                "DataFrames make it easy to analyze and export data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset Info:\n",
                        "<class 'pandas.core.frame.DataFrame'>\n",
                        "RangeIndex: 60 entries, 0 to 59\n",
                        "Data columns (total 6 columns):\n",
                        " #   Column        Non-Null Count  Dtype  \n",
                        "---  ------        --------------  -----  \n",
                        " 0   title         60 non-null     object \n",
                        " 1   price         60 non-null     float64\n",
                        " 2   rating        60 non-null     int64  \n",
                        " 3   availability  60 non-null     object \n",
                        " 4   image_url     60 non-null     object \n",
                        " 5   detail_url    60 non-null     object \n",
                        "dtypes: float64(1), int64(1), object(4)\n",
                        "memory usage: 2.9+ KB\n",
                        "None\n",
                        "\n",
                        "First 5 rows:\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>title</th>\n",
                            "      <th>price</th>\n",
                            "      <th>rating</th>\n",
                            "      <th>availability</th>\n",
                            "      <th>image_url</th>\n",
                            "      <th>detail_url</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>A Light in the Attic</td>\n",
                            "      <td>51.77</td>\n",
                            "      <td>3</td>\n",
                            "      <td>In stock</td>\n",
                            "      <td>https://books.toscrape.com/media/cache/2c/da/2...</td>\n",
                            "      <td>https://books.toscrape.com/catalogue/a-light-i...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Tipping the Velvet</td>\n",
                            "      <td>53.74</td>\n",
                            "      <td>1</td>\n",
                            "      <td>In stock</td>\n",
                            "      <td>https://books.toscrape.com/media/cache/26/0c/2...</td>\n",
                            "      <td>https://books.toscrape.com/catalogue/tipping-t...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Soumission</td>\n",
                            "      <td>50.10</td>\n",
                            "      <td>1</td>\n",
                            "      <td>In stock</td>\n",
                            "      <td>https://books.toscrape.com/media/cache/3e/ef/3...</td>\n",
                            "      <td>https://books.toscrape.com/catalogue/soumissio...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Sharp Objects</td>\n",
                            "      <td>47.82</td>\n",
                            "      <td>4</td>\n",
                            "      <td>In stock</td>\n",
                            "      <td>https://books.toscrape.com/media/cache/32/51/3...</td>\n",
                            "      <td>https://books.toscrape.com/catalogue/sharp-obj...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>Sapiens: A Brief History of Humankind</td>\n",
                            "      <td>54.23</td>\n",
                            "      <td>5</td>\n",
                            "      <td>In stock</td>\n",
                            "      <td>https://books.toscrape.com/media/cache/be/a5/b...</td>\n",
                            "      <td>https://books.toscrape.com/catalogue/sapiens-a...</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                   title  price  rating availability  \\\n",
                            "0                   A Light in the Attic  51.77       3     In stock   \n",
                            "1                     Tipping the Velvet  53.74       1     In stock   \n",
                            "2                             Soumission  50.10       1     In stock   \n",
                            "3                          Sharp Objects  47.82       4     In stock   \n",
                            "4  Sapiens: A Brief History of Humankind  54.23       5     In stock   \n",
                            "\n",
                            "                                           image_url  \\\n",
                            "0  https://books.toscrape.com/media/cache/2c/da/2...   \n",
                            "1  https://books.toscrape.com/media/cache/26/0c/2...   \n",
                            "2  https://books.toscrape.com/media/cache/3e/ef/3...   \n",
                            "3  https://books.toscrape.com/media/cache/32/51/3...   \n",
                            "4  https://books.toscrape.com/media/cache/be/a5/b...   \n",
                            "\n",
                            "                                          detail_url  \n",
                            "0  https://books.toscrape.com/catalogue/a-light-i...  \n",
                            "1  https://books.toscrape.com/catalogue/tipping-t...  \n",
                            "2  https://books.toscrape.com/catalogue/soumissio...  \n",
                            "3  https://books.toscrape.com/catalogue/sharp-obj...  \n",
                            "4  https://books.toscrape.com/catalogue/sapiens-a...  "
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Create DataFrame\n",
                "df = pd.DataFrame(all_books)\n",
                "\n",
                "# Display basic info\n",
                "print(\"Dataset Info:\")\n",
                "print(df.info())\n",
                "print(\"\\nFirst 5 rows:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Step 14: Data Analysis\n",
                "\n",
                "Let's analyze the scraped data!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Price Statistics:\n",
                        "count    60.000000\n",
                        "mean     35.002667\n",
                        "std      14.553082\n",
                        "min      12.840000\n",
                        "25%      22.040000\n",
                        "50%      33.485000\n",
                        "75%      50.142500\n",
                        "max      57.310000\n",
                        "Name: price, dtype: float64\n",
                        "\n",
                        "Rating Distribution:\n",
                        "rating\n",
                        "1    15\n",
                        "2     8\n",
                        "3    13\n",
                        "4    10\n",
                        "5    14\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Most Expensive Books:\n",
                        "                                                title  price  rating\n",
                        "40                     Slow States of Collapse: Poems  57.31       3\n",
                        "15  Our Band Could Be Your Life: Scenes from the A...  57.25       3\n",
                        "58                                The Past Never Ends  56.50       4\n",
                        "57  The Pioneer Woman Cooks: Dinnertime: Comfort C...  56.41       1\n",
                        "56                    The Secret of Dreadwillow Carse  56.13       1\n",
                        "\n",
                        "Cheapest Books:\n",
                        "                                                title  price  rating\n",
                        "20                                        In Her Wake  12.84       1\n",
                        "10     Starving Hearts (Triangular Trade Trilogy, #1)  13.99       2\n",
                        "47            Untitled Collection: Sabbath Poems 2014  14.27       4\n",
                        "34                                     Sophie's World  15.94       5\n",
                        "51  Tsubasa: WoRLD CHRoNiCLE 2 (Tsubasa WoRLD CHRo...  16.28       1\n"
                    ]
                }
            ],
            "source": [
                "# Price statistics\n",
                "print(\"Price Statistics:\")\n",
                "print(df['price'].describe())\n",
                "\n",
                "print(\"\\nRating Distribution:\")\n",
                "print(df['rating'].value_counts().sort_index())\n",
                "\n",
                "print(\"\\nMost Expensive Books:\")\n",
                "print(df.nlargest(5, 'price')[['title', 'price', 'rating']])\n",
                "\n",
                "print(\"\\nCheapest Books:\")\n",
                "print(df.nsmallest(5, 'price')[['title', 'price', 'rating']])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üíæ Step 15: Save Data to CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Data saved to books_data.csv\n"
                    ]
                }
            ],
            "source": [
                "# Save to CSV\n",
                "df.to_csv('books_data.csv', index=False)\n",
                "print(\"‚úì Data saved to books_data.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Step 16: Save Data to JSON"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì Data saved to books_data.json\n"
                    ]
                }
            ],
            "source": [
                "# Save to JSON\n",
                "df.to_json('books_data.json', orient='records', indent=2)\n",
                "print(\"‚úì Data saved to books_data.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîç Step 17: Scrape Individual Book Details\n",
                "\n",
                "Let's scrape more detailed information from individual book pages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scraping details for: A Light in the Attic\n",
                        "URL: https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\n",
                        "\n",
                        "Book Details:\n",
                        "UPC: a897fe39b1053632\n",
                        "Product Type: Books\n",
                        "Price (excl. tax): √Ç¬£51.77\n",
                        "Price (incl. tax): √Ç¬£51.77\n",
                        "Tax: √Ç¬£0.00\n",
                        "Availability: In stock (22 available)\n",
                        "Number of reviews: 0\n",
                        "Description: It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\n",
                        "Category: Poetry\n"
                    ]
                }
            ],
            "source": [
                "def scrape_book_details(book_url):\n",
                "    \"\"\"\n",
                "    Scrape detailed information from a book's detail page.\n",
                "    \n",
                "    Args:\n",
                "        book_url (str): URL of the book detail page\n",
                "    \n",
                "    Returns:\n",
                "        dict: Dictionary containing detailed book information\n",
                "    \"\"\"\n",
                "    response = requests.get(book_url)\n",
                "    soup = BeautifulSoup(response.text, 'lxml')\n",
                "    \n",
                "    # Extract product information table\n",
                "    table = soup.find('table', class_='table table-striped')\n",
                "    \n",
                "    details = {}\n",
                "    \n",
                "    # Extract UPC, Product Type, Price (excl. tax), etc.\n",
                "    for row in table.find_all('tr'):\n",
                "        header = row.find('th').get_text()\n",
                "        value = row.find('td').get_text()\n",
                "        details[header] = value\n",
                "    \n",
                "    # Extract description\n",
                "    description_tag = soup.find('div', id='product_description')\n",
                "    if description_tag:\n",
                "        description = description_tag.find_next('p').get_text()\n",
                "        details['Description'] = description\n",
                "    else:\n",
                "        details['Description'] = 'No description available'\n",
                "    \n",
                "    # Extract category\n",
                "    breadcrumb = soup.find('ul', class_='breadcrumb')\n",
                "    category = breadcrumb.find_all('a')[2].get_text(strip=True)\n",
                "    details['Category'] = category\n",
                "    \n",
                "    return details\n",
                "\n",
                "# Test with first book\n",
                "first_book_url = all_books[0]['detail_url']\n",
                "print(f\"Scraping details for: {all_books[0]['title']}\")\n",
                "print(f\"URL: {first_book_url}\\n\")\n",
                "\n",
                "book_details = scrape_book_details(first_book_url)\n",
                "print(\"Book Details:\")\n",
                "for key, value in book_details.items():\n",
                "    print(f\"{key}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üé® Step 18: Advanced - Scrape by Category\n",
                "\n",
                "The website organizes books by categories. Let's scrape a specific category."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_categories(base_url):\n",
                "    \"\"\"\n",
                "    Get all book categories from the website.\n",
                "    \n",
                "    Returns:\n",
                "        dict: Dictionary mapping category names to URLs\n",
                "    \"\"\"\n",
                "    response = requests.get(base_url)\n",
                "    soup = BeautifulSoup(response.text, 'lxml')\n",
                "    \n",
                "    # Find category sidebar\n",
                "    category_list = soup.find('ul', class_='nav nav-list').find('ul')\n",
                "    \n",
                "    categories = {}\n",
                "    for link in category_list.find_all('a'):\n",
                "        category_name = link.get_text(strip=True)\n",
                "        category_url = urljoin(base_url, link['href'])\n",
                "        categories[category_name] = category_url\n",
                "    \n",
                "    return categories\n",
                "\n",
                "# Get all categories\n",
                "categories = get_categories(url)\n",
                "print(f\"Found {len(categories)} categories:\\n\")\n",
                "for i, (name, url) in enumerate(list(categories.items())[:10], 1):\n",
                "    print(f\"{i}. {name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üõ°Ô∏è Step 19: Best Practices & Ethics\n",
                "\n",
                "### ‚úÖ DO:\n",
                "- Check `robots.txt` (https://books.toscrape.com/robots.txt)\n",
                "- Add delays between requests (`time.sleep()`)\n",
                "- Use User-Agent headers\n",
                "- Handle errors gracefully\n",
                "- Respect rate limits\n",
                "\n",
                "### ‚ùå DON'T:\n",
                "- Scrape personal data without permission\n",
                "- Overload servers with too many requests\n",
                "- Ignore Terms of Service\n",
                "- Scrape copyrighted content for commercial use"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Adding User-Agent header\n",
                "headers = {\n",
                "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
                "}\n",
                "\n",
                "response = requests.get(url, headers=headers)\n",
                "print(f\"Request with User-Agent: {response.status_code}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéØ Step 20: Complete Project - Scrape All Books\n",
                "\n",
                "Let's put it all together!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def complete_scraping_project(base_url, max_pages=5, include_details=False):\n",
                "    \"\"\"\n",
                "    Complete web scraping project.\n",
                "    \n",
                "    Args:\n",
                "        base_url (str): Base URL of the website\n",
                "        max_pages (int): Number of pages to scrape\n",
                "        include_details (bool): Whether to scrape individual book details\n",
                "    \n",
                "    Returns:\n",
                "        pd.DataFrame: DataFrame containing all scraped data\n",
                "    \"\"\"\n",
                "    print(\"üï∑Ô∏è Starting Web Scraping Project...\\n\")\n",
                "    \n",
                "    # Step 1: Scrape all pages\n",
                "    print(f\"üìÑ Scraping {max_pages} pages...\")\n",
                "    all_books = scrape_all_pages(base_url, max_pages)\n",
                "    print(f\"‚úì Scraped {len(all_books)} books\\n\")\n",
                "    \n",
                "    # Step 2: Convert to DataFrame\n",
                "    df = pd.DataFrame(all_books)\n",
                "    \n",
                "    # Step 3: Optionally scrape details\n",
                "    if include_details:\n",
                "        print(\"üìñ Scraping individual book details...\")\n",
                "        details_list = []\n",
                "        for i, book in enumerate(all_books[:10], 1):  # Limit to 10 for demo\n",
                "            print(f\"  {i}/10: {book['title'][:50]}...\")\n",
                "            details = scrape_book_details(book['detail_url'])\n",
                "            details_list.append(details)\n",
                "            time.sleep(1)\n",
                "        \n",
                "        # Merge details with main DataFrame\n",
                "        details_df = pd.DataFrame(details_list)\n",
                "        df = pd.concat([df.iloc[:10], details_df], axis=1)\n",
                "    \n",
                "    # Step 4: Save data\n",
                "    print(\"\\nüíæ Saving data...\")\n",
                "    df.to_csv('complete_books_data.csv', index=False)\n",
                "    df.to_json('complete_books_data.json', orient='records', indent=2)\n",
                "    print(\"‚úì Data saved to CSV and JSON\\n\")\n",
                "    \n",
                "    # Step 5: Display summary\n",
                "    print(\"üìä Summary Statistics:\")\n",
                "    print(f\"Total Books: {len(df)}\")\n",
                "    print(f\"Average Price: ¬£{df['price'].mean():.2f}\")\n",
                "    print(f\"Price Range: ¬£{df['price'].min():.2f} - ¬£{df['price'].max():.2f}\")\n",
                "    print(f\"Average Rating: {df['rating'].mean():.2f} stars\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Run the complete project\n",
                "final_df = complete_scraping_project(url, max_pages=3, include_details=False)\n",
                "final_df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üéì Summary\n",
                "\n",
                "### What You Learned:\n",
                "\n",
                "1. ‚úÖ **HTTP Requests** - Fetching web pages with `requests`\n",
                "2. ‚úÖ **HTML Parsing** - Using BeautifulSoup to navigate DOM\n",
                "3. ‚úÖ **Element Selection** - `.find()`, `.find_all()`, CSS selectors\n",
                "4. ‚úÖ **Data Extraction** - Text, attributes, images, links\n",
                "5. ‚úÖ **Data Cleaning** - Handling encoding issues with regex\n",
                "6. ‚úÖ **Pagination** - Scraping multiple pages\n",
                "7. ‚úÖ **Data Organization** - Pandas DataFrames\n",
                "8. ‚úÖ **Data Export** - CSV and JSON formats\n",
                "9. ‚úÖ **Best Practices** - Delays, error handling, ethics\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "- üîß Try scraping other websites\n",
                "- üìä Add data visualization (matplotlib, seaborn)\n",
                "- üóÑÔ∏è Store data in databases (SQLite, PostgreSQL)\n",
                "- ü§ñ Automate with scheduling (cron, Task Scheduler)\n",
                "- üåê Learn Selenium for JavaScript-heavy sites\n",
                "\n",
                "---\n",
                "\n",
                "**Happy Scraping! üï∑Ô∏è**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
